{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIT 6.036 Lab11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ToniRV/MIT_6.862_Applied_Machine_Learning/blob/master/MIT_6_036_Lab11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XozwtJgh1-u",
        "colab_type": "text"
      },
      "source": [
        "# **MIT 6.036 Spring 2020: Lab 11**\n",
        "\n",
        "This colab notebook provides code and a framework for sections 2 and 3 of the lab.\n",
        "\n",
        "## **Setup**\n",
        "\n",
        "First, download the code distribution for this homework that contains test cases and helper functions.\n",
        "\n",
        "Run the next code block to download and import the code for this lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCJPrb3KhPmw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "419df1ca-78cd-498f-cc97-c763edeae6c2"
      },
      "source": [
        "!rm -rf code_for_lab11*\n",
        "!wget --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/labs/lab11/code_for_lab_11.zip\n",
        "!unzip code_for_lab_11.zip\n",
        "!mv ./code_for_lab11/* ./"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  code_for_lab_11.zip\n",
            "   creating: code_for_lab11/\n",
            "  inflating: code_for_lab11/code_for_lab11.py  \n",
            "  inflating: code_for_lab11/metal_bands.txt  \n",
            "  inflating: code_for_lab11/.DS_Store  \n",
            "  inflating: code_for_lab11/util.py  \n",
            "  inflating: code_for_lab11/companies.csv  \n",
            "   creating: code_for_lab11/models/\n",
            "  inflating: code_for_lab11/models/metal_rnn.p  \n",
            "  inflating: code_for_lab11/models/MIT_classes_rnn.p  \n",
            "  inflating: code_for_lab11/models/food_rnn.p  \n",
            "  inflating: code_for_lab11/models/companies_rnn.p  \n",
            "  inflating: code_for_lab11/sm.py    \n",
            "  inflating: code_for_lab11/baskervilles.txt  \n",
            "  inflating: code_for_lab11/MIT_classes.txt  \n",
            "  inflating: code_for_lab11/basicEnglish.txt  \n",
            "  inflating: code_for_lab11/simple_poem.txt  \n",
            "  inflating: code_for_lab11/food.txt  \n",
            "  inflating: code_for_lab11/mousquetaires.txt  \n",
            "  inflating: code_for_lab11/companies.txt  \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._code_for_lab11  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5FLPgjtrnU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sm import *\n",
        "from util import *\n",
        "from code_for_lab11 import *\n",
        "import numpy as np\n",
        "import _pickle as cPickle\n",
        "m = cPickle.load(open('models/food_rnn.p', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOp2ZR1Nlc92",
        "colab_type": "text"
      },
      "source": [
        "## **Section 2, Problem B**\n",
        "\n",
        "Below, you will find definitions of a procedure called *test_linear_accumulator* for training an rnn with input and output sequences of the kind produced by Accumulator. (Alternatively, you can look at *code_for_lab11.py* of the code file available for download) Study this function, in particular, the definition of the RNN instance; compare to your choices in the previous question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otw09SGC1gk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# please evaluate this cell to see its output\n",
        "\n",
        "def test_linear_accumulator(num_steps = 10000,\n",
        "                            num_seqs = 400, seq_length = 40,\n",
        "                            step_size = .01):\n",
        "    # generate random training data: num_seqs of seq_length of random\n",
        "    # numbers between -0.5 and 0.5.\n",
        "    data = []\n",
        "    for _ in range(num_seqs):           \n",
        "        x = np.random.random((1, seq_length)) - 0.5 # seq in\n",
        "        y = np.zeros((1, seq_length))               # seq out\n",
        "        for j in range(seq_length):\n",
        "            y[0, j] = x[0, j] + (0.0 if j == 0 else y[0, j-1])\n",
        "        data.append((x, y))\n",
        "    # specify rnn\n",
        "    rnn = RNN(1, 1, 1, quadratic_loss, lambda z: z, quadratic_linear_gradient,\n",
        "              step_size, lambda z: z, lambda z: 1)\n",
        "    # train it\n",
        "    rnn.train_seq_to_seq(data, num_steps)\n",
        "    # print weights\n",
        "    print(\"Wsx: \", rnn.Wsx); print(\"Wss: \", rnn.Wss); print(\"Wo: \", rnn.Wo); print(\"Wss0: \", rnn.Wss0); print(\"Wo0: \", rnn.Wo0)\n",
        "    return rnn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-OD7bEq1pSL",
        "colab_type": "text"
      },
      "source": [
        "Now run this function a few times, making sure that the training error is low and look at the final weights. Relate to the weights you chose above. Explain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t1QR46mE1Vzj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "19a1eeda-98b1-44fb-b01f-11bc35bd0b4d"
      },
      "source": [
        "test_linear_accumulator()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/10: training error 0.10965130122299203\n",
            "2/10: training error 0.08017097075653189\n",
            "3/10: training error 2.386918447160652e-05\n",
            "4/10: training error 0.004187864759620643\n",
            "5/10: training error 0.0014861850326898364\n",
            "6/10: training error 0.07683477789102473\n",
            "7/10: training error 1.2740239379311168e-11\n",
            "8/10: training error 2.0362360289305708e-16\n",
            "9/10: training error 0.04580439389405539\n",
            "Wsx:  [[2.42525127]]\n",
            "Wss:  [[0.99619299]]\n",
            "Wo:  [[0.40954527]]\n",
            "Wss0:  [[0.00046585]]\n",
            "Wo0:  [[-0.00218033]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<code_for_lab11.RNN at 0x7f57db50b6a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ629z-1xz4d",
        "colab_type": "text"
      },
      "source": [
        "## **Section 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4RKu7cNesji",
        "colab_type": "text"
      },
      "source": [
        "### Generating sequences\n",
        "\n",
        "We are interested in building an RNN model that can predict the next\n",
        "element in a sequence (this is sometimes referred to as a \"language\"\n",
        "model).    The particular form of the model that we will look at is:\n",
        "\n",
        "<displaymath>\n",
        "\\begin{align}\n",
        "x_t &= \\phi(c_{t-1})\\\\\n",
        "s_t &= \\tanh(W^{ss} s_{t-1} + W^{sx} x_t + W^{ss}_0)\\\\\n",
        "p_t &= \\text{softmax}(W^o s_t + W^o_0)\n",
        "\\end{align}\n",
        "</displaymath>\n",
        "\n",
        "**Please refer to the lab's prompt for more definitions and details about this section.**\n",
        "\n",
        "As we just said, you can find more details in the lab's prompt, but here some extra details:\n",
        "\n",
        "Training is as follows:\n",
        "<ul>\n",
        "<li> For each sequence in the input data, it feeds in character <math>t-1</math> from\n",
        "  the training data and predicts character $t$.  </li>\n",
        "</ul>\n",
        "\n",
        "Generation is as follows:\n",
        "<ul>\n",
        "<li> Starting with the `start` symbol ('.'), it predicts a next\n",
        "  character based on the softmax distribution in the trained model,\n",
        "  then it feeds that character into the model and repeats until an\n",
        "  <i>end</i> symbol ('\\n') is generated.</li>\n",
        "</ul>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtbW_gHiyyAq",
        "colab_type": "text"
      },
      "source": [
        "For each of \n",
        "\n",
        "*   \"aaaaaaaaaa\"\n",
        "*   \"aabaaabbaaaababaabaa\"\n",
        "*   \"abcdefghijklmnopqrstuvwxyz\"\n",
        "*   \"abcabcabcabcabc\"\n",
        "\n",
        "train an RNN and assess the difficulty of learning each string. You may control *num_hidden* and *num_steps* to facilitate your training in the code below.\n",
        "\n",
        "**Note**: The output of `test_word` is 100 sequences produced by running the generation process on a trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euRE-Zglzket",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "570b750d-95ac-4838-d00f-c54250e15986"
      },
      "source": [
        "test_word(word=\"aaaaaaaaaa\", interactive = False, num_hidden=1, num_steps=10000, step_size=0.005)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/10: training error 0.3058238806070037\n",
            "2/10: training error 0.03325205127086638\n",
            "3/10: training error 0.008949570684933563\n",
            "4/10: training error 0.0038594599359628914\n",
            "5/10: training error 0.0018758980894445115\n",
            "6/10: training error 0.04337550443789023\n",
            "7/10: training error 0.002063590198645276\n",
            "8/10: training error 0.0013923343442130978\n",
            "9/10: training error 0.0009078512621424098\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n",
            "'.aaaaaaaaaa\\n'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<code_for_lab11.RNN at 0x7f57db50bb38>,\n",
              " <code_for_lab11.OneHotCodec at 0x7f57db50bac8>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--_tiGbyzoCW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "89ef2c0d-4d82-448b-8eb0-1f164aa1be03"
      },
      "source": [
        "test_word(word=\"aabaaabbaaaababaabaa\", interactive = False, num_hidden=5, num_steps=20000, step_size=0.005)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/10: training error 0.3562842395495819\n",
            "2/10: training error 0.17663127472291945\n",
            "3/10: training error 0.05366542651599676\n",
            "4/10: training error 0.0040842361912864595\n",
            "5/10: training error 0.0012079256798835163\n",
            "6/10: training error 0.0003862590087258371\n",
            "7/10: training error 0.0001259288928377182\n",
            "8/10: training error 4.149626661477658e-05\n",
            "9/10: training error 1.3794086659829036e-05\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n",
            "'.aabaaabbaaaababaabaa\\n'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<code_for_lab11.RNN at 0x7f57db503d30>,\n",
              " <code_for_lab11.OneHotCodec at 0x7f57db503dd8>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGAyQxO7zqSG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d58d70fb-ea45-4039-83ff-01e50e483d66"
      },
      "source": [
        "test_word(word=\"abcdefghijklmnopqrstuvwxyz\", interactive = False, num_hidden=28, num_steps=100, step_size=0.005)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/10: training error 3.611447829999854\n",
            "2/10: training error 3.135614837216415\n",
            "3/10: training error 2.5192144928880924\n",
            "4/10: training error 1.5501658742339859\n",
            "5/10: training error 0.7822380661404886\n",
            "6/10: training error 0.3524361121429647\n",
            "7/10: training error 0.1616303609148579\n",
            "8/10: training error 0.08686653836513578\n",
            "9/10: training error 0.05582428347011883\n",
            "'.abcdefghijklmnosqrstuvwxyz\\n'\n",
            "'.abcdbfghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdifghijklmnopqrstuvwxyc\\n'\n",
            "'.abcdefghijkomnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefgyijklmnopqrstuvwxyz\\n'\n",
            "'.abtdgfghijklmnopqrvtuvwxyz\\n'\n",
            "'.abcuefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmqqpqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abbretuvwl\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvaxyz\\n'\n",
            "'.abcdefghijkhmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrgtuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrbcuvwxyz\\n'\n",
            "'.fbcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnobqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyw\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefgwijklmqopqrstuvwxyz\\n'\n",
            "'.abcdefghujmlgnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghilklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijkjmnopqrstuvwxyz\\n'\n",
            "'.abcdefghejklmnoporstuvhxyz\\n'\n",
            "'.abcdefghijklmnopqrptuvwgyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdeaghijklmnopqrstzvwgyzsqdscd\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcugfghwjklmaopqrstuvwxyz\\n'\n",
            "'.abcdefghijqlmnopqrstuvwxyz\\n'\n",
            "'.abcdefghilklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrscuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghifklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnlpqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijkhmnopqrstuvwxyz\\n'\n",
            "'.obcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghilklmnopqrvtuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstrvwxhz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghljklmnopqrstuvwxez\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnosqrstuvwxyz\\n'\n",
            "'.abcdefghixklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrscuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyzvipqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnrpqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefxhijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefxhijklmnopqrstuvwxyz\\n'\n",
            "'.pbcdefghijklmnopqrstuvwxyzaqrstuvfpyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwgyz\\n'\n",
            "'.abcdefghijklmnrpqrvtuvwxyz\\n'\n",
            "'.abcdefgyijklmnopqustuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnoptrstxvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijjlmnopqrstuvwxyzsrpqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrptuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefxhijklmnopnrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuwwgyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwgyz\\n'\n",
            "'.abcd\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abndefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvfxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrsquvwxyz\\n'\n",
            "'.abcdefghijkopnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuywxyz\\n'\n",
            "'.abcdvfghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyu\\n'\n",
            "'.abcdewghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnlpqrstuvwxtz\\n'\n",
            "'.abcdefghijklmnopqrstuvwxyz\\n'\n",
            "'.abcdefghijklmnoptrstuvwxyz\\n'\n",
            "'.abcdefgyijklmnopqrstuvwgyz\\n'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<code_for_lab11.RNN at 0x7f57db5038d0>,\n",
              " <code_for_lab11.OneHotCodec at 0x7f57db503c88>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BI7pgDbzsim",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9af3022-f592-4bd3-e2b4-bedec0d5880b"
      },
      "source": [
        "test_word(word=\"abcabcabcabcabc\", interactive = False, num_hidden=1, num_steps=10000, step_size=0.005)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/10: training error 0.6241473815778071\n",
            "2/10: training error 0.27025810189508526\n",
            "3/10: training error 0.20989295950357387\n",
            "4/10: training error 0.18928764357001288\n",
            "5/10: training error 0.18008102213384938\n",
            "6/10: training error 0.17536219198461328\n",
            "7/10: training error 0.17275233920239447\n",
            "8/10: training error 0.17124376356099869\n",
            "9/10: training error 0.17034797918060607\n",
            "'.abc\\n'\n",
            "'.abc\\n'\n",
            "'.abcabcabcabcabc\\n'\n",
            "'.abcabcabcabc\\n'\n",
            "'.abcabcabcabcabc\\n'\n",
            "'.abc\\n'\n",
            "'.abcabcabcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabcabc\\n'\n",
            "'.abc\\n'\n",
            "'.abcabcabcabcabc\\n'\n",
            "'.abcabc\\n'\n",
            "'.abcabcabcabcabcabc\\n'\n",
            "'.abc\\n'\n",
            "'.abcabcabc\\n'\n",
            "'.abcabc\\n'\n",
            "'.abcabcabcabc\\n'\n",
            "'.abc\\n'\n",
            "'.abcabc\\n'\n",
            "'.abcabcabcabcabcabcabcabcabcabcbcabcabcabc\\n'\n",
            "'.abc\\n'\n",
            "'.abc\\n'\n",
            "'.abcabc\\n'\n",
            "'.abcabcabc\\n'\n",
            "'.abcabcabcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabcabc\\n'\n",
            "'.abcabcabc\\n'\n",
            "'.abcabcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabcabcabc\\n'\n",
            "'.abcabc\\n'\n",
            "'.abcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabcabcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabcabc\\n'\n",
            "'.abcabcabc\\n'\n",
            "'.abc\\n'\n",
            "'.abcabcabcabcabcabcabcabcabcabcabc\\n'\n",
            "'.abcabcabc\\n'\n",
            "'.abcabcabcabcabcabcabc\\n'\n",
            "'.\\n'\n",
            "'.abcabc\\n'\n",
            "'.abcabcabcabcabcabcabcabcabcabc\\n'\n",
            "'.abc\\n'\n",
            "'.abcabc\\n'\n",
            "'.abcabcabcabcabcabcabcabc\\n'\n",
            "'.abcabcabcabc\\n'\n",
            "'.abcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabcabcabcabcabcabcabcabc\\n'\n",
            "'.abcabcabcabc\\n'\n",
            "'.abcabcabcabcabcabcabcabcabcabc\\n'\n",
            "'.abcabcabcabc\\n'\n",
            "'.abcabcabc\\n'\n",
            "'.abcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabcabc\\n'\n",
            "'.abc\\n'\n",
            "'.abcabcabcabcabc\\n'\n",
            "'.abc\\n'\n",
            "'.abcabcabcabcabcabc\\n'\n",
            "'.abc\\n'\n",
            "'.abcabcbcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabcabcabcabcabcabcabcabcabc\\n'\n",
            "'.abcabcabc\\n'\n",
            "'.abcabcabcabcabcabcabcabc\\n'\n",
            "'.abcabc\\n'\n",
            "'.abcabc\\n'\n",
            "'.abcabc\\n'\n",
            "'.abcabcabc\\n'\n",
            "'.abcabcabcabc\\n'\n",
            "'.abcabcabcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabc\\n'\n",
            "'.abcabcac\\n'\n",
            "'.abcabcabcabcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabcabcabcabcabcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabcabcabcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabcabcabcabcabcabcabcabcabc\\n'\n",
            "'.abcabcabcabc\\n'\n",
            "'.abc\\n'\n",
            "'.abcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabcabcabcabcabcabcabcabcabc\\n'\n",
            "'.abcabc\\n'\n",
            "'.abcabcabcabcabcabcabcabcabcabc\\n'\n",
            "'.abcabcabcabcabcabcabcabcabc\\n'\n",
            "'.abc\\n'\n",
            "'.abc\\n'\n",
            "'.abcabcabcabcabcabcabcabcabc\\n'\n",
            "'.abcabcabcabc\\n'\n",
            "'.abc\\n'\n",
            "'.abcabcabcabcabcabcabc\\n'\n",
            "'.abcabcabcabc\\n'\n",
            "'.abcabc\\n'\n",
            "'.abcabcabcabcabcabcabcabcabcabcabcabc\\n'\n",
            "'.abcabcabc\\n'\n",
            "'.abcabcabcabcabc\\n'\n",
            "'.abcabcabc\\n'\n",
            "'.abcabc\\n'\n",
            "'.abcabcabcabc\\n'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<code_for_lab11.RNN at 0x7f57db518240>,\n",
              " <code_for_lab11.OneHotCodec at 0x7f57db503668>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuA9fE44zdUb",
        "colab_type": "text"
      },
      "source": [
        "## Problem 3D\n",
        "\n",
        "The *test_company_names* function uses a file <a href=\"https://docs.google.com/spreadsheets/d/16wbhwRwu5AP4gydX9buHZj884v48HURaalYtKmdT8r0/edit?usp=sharing\">companies.txt</a> of company names for training and generates new names.\n",
        "\n",
        "Experiment with different values of `num_steps`; more steps gives better results. Try it in interactive mode; it's more fun.\n",
        "\n",
        "**In running `test_company_names`, first try with `train=True`; you can train the RNN yourself with this option. If this takes too much time, run  `test_company_names` with `train=False`; you can simply load a pre-trained model with this option.**\n",
        "\n",
        "Please note that when we train our models, we need to decide what will be our vocabulary size (i.e. how many classes/characters/words our model will learn to predict). Hence, it is possible that when you input a character/word that is out of that vocabulary (depending on how the model is trained -- character-based or word-based), the model crashes (in more sophisticated solutions we could use a `<unknown>` token to represent **tokens out of vocabulary.**)\n",
        "\n",
        "These models are trained by minimizing NLL.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj8wsxKr18Mj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "545406a2-68f3-4e8e-818c-2fe686f2d242"
      },
      "source": [
        "train = False\n",
        "\n",
        "test_company_names(interactive = True, interactive_top5 = False,\n",
        "                   train=train)\n",
        "\n",
        "#Trained with num_hidden = 150, num_steps = 20000,\n",
        "#             step_size = .001, split=0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Type in a start-of-sequence string\n",
            "(the model will try to generate the rest): appl\n",
            "'.applers\\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEwfEUrzx2zr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This cell may take a long time because here we are training a model!\n",
        "train = True\n",
        "\n",
        "test_company_names(interactive = False, interactive_top5 = False, \n",
        "           split=0, num_hidden = 150, \n",
        "           num_steps = 20000, step_size = .001, train=train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxC8sgJV0WOP",
        "colab_type": "text"
      },
      "source": [
        "## Problem 3E\n",
        "\n",
        "The *test_food* function uses a file <a href=\"https://docs.google.com/spreadsheets/d/12eomCfekSdqTlOB4Xwp3n7_FN0SVdVrTDLVUBq-1jyw/edit?usp=sharing\">food.txt</a> of recipe names. Experiment with different values of num_steps; more steps gives better results. Try it in interactive mode; it's more fun. Note the difference between starting with a capital letter versus lower case letter.\n",
        "\n",
        "**In running `test_food`, first try with `train=True`; you can train the RNN yourself with this option. If this takes too much time, run  `test_food` with `train=False`; you can simply load a pre-trained model with this option.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-G-bm0p1_FT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = False\n",
        "\n",
        "test_food(interactive = True, interactive_top5 = False, \n",
        "          train=train)\n",
        "\n",
        "#Trained with num_hidden = 150, num_steps = 20000,\n",
        "#             step_size = .001, split=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHRpZudK0iye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This cell may take a long time because here we are training a model!\n",
        "train = True\n",
        "\n",
        "test_food(interactive = True, interactive_top5 = False, \n",
        "          split=0, num_hidden = 150, \n",
        "          num_steps = 20000, step_size = .001, train=train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFYBVV4Svfsh",
        "colab_type": "text"
      },
      "source": [
        "## Problem 3F\n",
        "\n",
        "Now run one of <code>test_food, test_company_names</code> with <code>interactive_top5=True</code>; play with them for a while. Can you notice the mechanism of how these functions generate their output? \n",
        "\n",
        "Use the cell below to run one of <code> test_food, test_company_names</code> with <code>interactive_top5=True</code>!\n",
        "\n",
        "- What is the mechanism by which these RNNs generate their output? More specifically, what character does the trained RNN seem to output at each location?\n",
        "- Also, what would be a reasonable criterion for the above-trained RNNs to stop generating more characters? When does it seem to stop? Be prepared to explain this during checkoff."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug4mvoGaVeKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = True\n",
        "\n",
        "test_food(interactive = True, interactive_top5 = True,\n",
        "          train=train)\n",
        "\n",
        "# Pre trained with split=0, num_hidden = 150,\n",
        "#                   num_steps = 20000, step_size = .001, "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3R3Hmu0xvht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = False\n",
        "\n",
        "test_company_names(interactive = True, interactive_top5 = True,\n",
        "                  train=train)\n",
        "\n",
        "# Pre trained with split=0, num_hidden = 150,\n",
        "#                   num_steps = 20000, step_size = .001, "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vamyQMx0foA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}