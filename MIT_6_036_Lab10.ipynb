{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIT 6.036 Lab10",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ToniRV/MIT_6.862_Applied_Machine_Learning/blob/master/MIT_6_036_Lab10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q58cS9antfCw",
        "colab_type": "text"
      },
      "source": [
        "#MIT 6.036 Spring 2020: Lab10#\n",
        "\n",
        "This colab notebook runs the companion code for question 3 in MIT 6.036 Lab 10. You can work out your solutions here, then submit your results back on the lab page when ready. If you have not used colab before, ask your partner or a TA for help.\n",
        "\n",
        "## <section>**Setup**</section>\n",
        "\n",
        "First, run the next code block to download and import the code for this lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUEtSZRdtmI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf code_for_lab10* __MACOSX data .DS_Store\n",
        "!wget --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/labs/lab10/code_for_lab10.zip\n",
        "!unzip code_for_lab10.zip\n",
        "!mv code_for_lab10/* .\n",
        "\n",
        "from code_for_lab10 import *\n",
        "\n",
        "import numpy as np\n",
        "import math as m\n",
        "import random\n",
        "\n",
        "import importlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M4mYROqdiGK",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Versus Reality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFo6mGJvdkct",
        "colab_type": "text"
      },
      "source": [
        "Once you run the code below wait patiently until you see a yellow and purple square moving around on a teal background.  Ignore everything else for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNN6wDVGdncW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try_tabular_batch_q_learning(iters=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VHQyT54dszA",
        "colab_type": "text"
      },
      "source": [
        "Click play in the button right below the square. This is a movie of a policy playing the game [No Exit](https://en.wikipedia.org/wiki/No_Exit). It’s kind of like Pong: the purple square is the “ball” and the yellow square is your “paddle”. The actions are to move the paddle up, down, or keep it still.\n",
        "\n",
        "The state is specified by the positions and velocities of the ball and paddle, with a special added “game over” state.\n",
        "\n",
        "The transition model is a very approximate physics model of the ball reflecting off walls and the paddle, except if the ball gets past the paddle in the positive x direction, the game is over.\n",
        "\n",
        "The agent gets a reward of +1 on every step it manages to survive.\n",
        "\n",
        "When watching the game play out, you’ll sometimes see that the purple square gets near the right-hand border and then suddenly it changes to a state with the purple square in the bottom left and the yellow one in the upper right -- this means that the game terminated and then reset to the initial state.\n",
        "\n",
        "Now we can go back and look at the other output in the notebook:\n",
        "\n",
        "* First, we show a table of what happens during learning: after every 20 iterations of batch Q learning, we take the current greedy policy and run it to see what its average score is. This score represents how long the episode ran before the ball ran off the map, or 100 if it lasted for that long.\n",
        "\n",
        "* Next is a plot of the score as a function of the amount of training.\n",
        "\n",
        "* Finally, we run the greedy policy with respect to the last Q-value function for 10 games and report the rewards achieved on each game. We also make a movie of these 10 games, which is what we started out looking at."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRwAr3O_wAEM",
        "colab_type": "text"
      },
      "source": [
        "**3.2)** Run the code given on the notebook for values of $\\epsilon$ in the set ${0,0.5,1}$. Does your observation match your answers from 3.1?\n",
        "\n",
        "\n",
        "Remember that this is a small instance, so sometimes the random noise of the environment might prevent you from seeing any useful information. Run the notebook two or three times if something doesn't line up with your expectation, and then ask for help."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb1bl_jCWVpY",
        "colab_type": "code",
        "outputId": "e9770fed-e229-434e-88d3-185c77aeba14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "try_tabular_batch_q_learning(iters=200, eps=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b35bd6c5ffd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtry_tabular_batch_q_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'try_tabular_batch_q_learning' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEj9VmCsWc1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try_tabular_batch_q_learning(iters=200, eps=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rL7_Z9sWffh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try_tabular_batch_q_learning(iters=200, eps=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keaYd0d4wG2_",
        "colab_type": "text"
      },
      "source": [
        "**(Optional)** Once you are done with the check off, play with the number of iterations in the colab until you get all of the methods to find a policy that scores 100 on all 10 of the final trials, and then observe the gameplay of the agents. You might expect the model that learned with $\\epsilon = 1$ has a more jittery gameplay than the agent that learned with $\\epsilon=0$ or $0.5$. Does that hold?\n"
      ]
    }
  ]
}