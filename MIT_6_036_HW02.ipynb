{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIT 6.036 HW02",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ToniRV/MIT_6.862_Applied_Machine_Learning/blob/master/MIT_6_036_HW02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xIaEwCD406A",
        "colab_type": "text"
      },
      "source": [
        "#MIT 6.036 Spring 2020: Homework 2#\n",
        "\n",
        "This colab notebook provides code and a framework for problems 8-10 of the homework.  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
        "\n",
        "## <section>**Setup**</section>\n",
        "\n",
        "First, download the code distribution for this homework that contains test cases and helper functions (such as `positive`).\n",
        "\n",
        "Run the next code block to download and import the code for this lab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YM-_zLf9Bp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -f code_for_hw02.py*\n",
        "!wget --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/homework/hw02/code_for_hw02.py\n",
        "from code_for_hw02 import *\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bhI4dQB1-UZ",
        "colab_type": "text"
      },
      "source": [
        "# 8) Implement Perceptron\n",
        "\n",
        "Implement [the Perceptron algorithm](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2020_Spring/courseware/Week2/perceptron/2), where\n",
        "\n",
        "* `data` is a numpy array of dimension $d$ by $n$\n",
        "* `labels` is numpy array of dimension $1$ by $n$\n",
        "* `params` is a dictionary specifying extra parameters to this algorithm; your algorithm should run a number of iterations equal to $T$\n",
        "* `hook` is either `None` or a function that takes the tuple `(th, th0)` as an argument and displays the separator graphically.  We won't be testing this in the Tutor, but it will help you in debugging on your own machine.\n",
        "\n",
        "It should return a tuple of $\\theta$ (a $d$ by 1 array) and $\\theta_0$ (a 1 by 1 array).\n",
        "\n",
        "We have given you some  data sets in the code file for you to test your implementation. Below are some test cases.\n",
        "```\n",
        "# Test Case 1\n",
        ">>> data = np.array([[2, 3, 9, 12],\n",
        "                     [5, 2, 6, 5]])\n",
        ">>> labels = np.array([[1, -1, 1, -1]])\n",
        ">>> [x.tolist() for x in perceptron(data, labels, {\"T\": 100})]\n",
        "[[[-24.0], [37.0]], [[-3.0]]]\n",
        "\n",
        "# Test Case 2\n",
        ">>> data = np.array([[1, 2, 1, 2],\n",
        "                     [1, 2, 2, 1]])\n",
        ">>> labels = np.array([[1, 1, -1, -1]])\n",
        ">>> [x.tolist() for x in perceptron(data, labels, {\"T\": 100})]\n",
        "[[[0.0], [-3.0]], [[0.0]]]\n",
        "```\n",
        "\n",
        "Your function should initialize any parameters defined in the function to 0, then run through the data, in the order it is given, performing an update to the parameters whenever the current parameters would make a mistake on that data point. Perform `T` iterations through the data. After every parameter update, if `hook` is defined, call it on the current `(th, th0)` (as a single parameter in a Python tuple)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtYf8ysk-VQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perceptron(data, labels, params={}, hook=None):\n",
        "    # if T not in params, default to 100\n",
        "    T = params.get('T', 100)\n",
        "\n",
        "    # Your implementation here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92r2oL42-yfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_perceptron(perceptron)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQMcSWlmB4-Y",
        "colab_type": "text"
      },
      "source": [
        "# 9) Implement Averaged Perceptron\n",
        "\n",
        "Regular perceptron can be somewhat sensitive to the most recent examples that it sees. Instead, averaged perceptron produces a more stable output by outputting the average value of `th` and `th0` across all iterations.\n",
        "\n",
        "Below are some test cases.\n",
        "```\n",
        "# Test Case 1\n",
        ">>> data = np.array([[2, 3, 9, 12],\n",
        "                     [5, 2, 6, 5]])\n",
        ">>> labels = np.array([[1, -1, 1, -1]])\n",
        ">>> [x.tolist() for x in averaged_perceptron(data, labels, {\"T\": 100})]\n",
        "[[[-22.1925], [34.06]], [[-2.1725]]]\n",
        "\n",
        "# Test Case 2\n",
        ">>> data = np.array([[1, 2, 1, 2],\n",
        "                     [1, 2, 2, 1]])\n",
        ">>> labels = np.array([[1, 1, -1, -1]])\n",
        ">>> [x.tolist() for x in averaged_perceptron(data, labels, {\"T\": 100})]\n",
        "[[[1.47], [-1.7275]], [[0.985]]]\n",
        "```\n",
        "\n",
        "\n",
        "Implement averaged perceptron with the same spec as regular perceptron, and using the pseudocode below as a guide.\n",
        "\n",
        "<pre>\n",
        "procedure averaged_perceptron({(x^(i), y^(i)), i=1,...n}, T)\n",
        "    th = 0 (d by 1); th0 = 0 (1 by 1)\n",
        "    ths = 0 (d by 1); th0s = 0 (1 by 1)\n",
        "    for t = 1,...,T do:\n",
        "        for i = 1,...,n do:\n",
        "\t        if y^(i)(th . x^(i) + th0) <= 0 then\n",
        "              th = th + y^(i)x^(i)\n",
        "              th0 = th0 + y^(i)\n",
        "\t        ths = ths + th\n",
        "\t        th0s = th0s + th0\n",
        "    return ths/(nT), th0s/(nT)\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAwW00MU_FzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def averaged_perceptron(data, labels, params={}, hook=None):\n",
        "    # if T not in params, default to 100\n",
        "    T = params.get('T', 100)\n",
        "    \n",
        "    # Your implementation here\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyLGH0_cBFSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_averaged_perceptron(averaged_perceptron)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTfGq7LNGceQ",
        "colab_type": "text"
      },
      "source": [
        "# 10) Implement Evaluation Strategies\n",
        "  \n",
        "## 10.1)  Evaluating a Classifier\n",
        "\n",
        "To evaluate a classifier, we are interested in how well it performs on data that it wasn't trained on. Construct a testing procedure that uses a training data set, calls a learning algorithm to get a linear separator (a tuple of $\\theta, \\theta_0$), and then reports the percentage correct on a new testing set as a float between $0.0$ and $1.0$.\n",
        "\n",
        "The learning algorithm is passed as a function that takes a data array and a labels vector.  Your evaluator should be able to interchangeably evaluate `perceptron` or `averaged_perceptron` (or future algorithms with the same spec), depending on what is passed through the `learner` parameter.\n",
        "\n",
        "Assume that you have available the function `score` from HW 1, which takes inputs:\n",
        "\n",
        "* `data`: a `d` by `n` array of floats (representing `n` data points in `d` dimensions)\n",
        "* `labels`: a `1` by `n` array of elements in `(+1, -1)`, representing target labels\n",
        "* `th`: a `d` by `1` array of floats that together with `th0`, represents a hyperplane\n",
        "* `th0`: a single scalar or `1` by `1` array\n",
        "\n",
        "and returns a scalar number of data points that the separator correctly classified.\n",
        "\n",
        "The `eval_classifier` function should accept the following parameters:\n",
        "\n",
        "* `learner` - a function, such as `perceptron` or `averaged_perceptron`\n",
        "* `data_train` - training data\n",
        "* `labels_train` - training labels\n",
        "* `data_test` - test data\n",
        "* `labels_test` - test labels\n",
        "\n",
        "and returns the percentage correct on a new testing set as a float between $0.0$ and $1.0$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSip1lfHBKaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_classifier(learner, data_train, labels_train, data_test, labels_test):\n",
        "    # Your implementation here\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beHMGAb6BTu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_eval_classifier(eval_classifier, perceptron)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WPStky3GiJb",
        "colab_type": "text"
      },
      "source": [
        "## 10.2) Evaluating a Learning algorithm using a data source\n",
        "\n",
        "Construct a testing procedure that takes a learning algorithm and a data source as input and runs the learning algorithm multiple times, each time evaluating the resulting classifier as above. It should report the overall average classification accuracy.\n",
        "\n",
        "You can use our implementation of `eval_classifier` as above.\n",
        "\n",
        "Write the function `eval_learning_alg` that takes:\n",
        "\n",
        "* `learner` - a function, such as `perceptron` or `averaged_perceptron`\n",
        "* `data_gen` - a data generator, call it with a desired data set size; returns a tuple `(data, labels)`\n",
        "* `n_train` - the size of the learning sets\n",
        "* `n_test` - the size of the test sets\n",
        "* `it` - the number of iterations to average over\n",
        "\n",
        "and returns the average classification accuracy as a float between $0.0$ and $1.0$.\n",
        "\n",
        "**Note: Be sure to generate your training data and then testing data in that order, to ensure that the pseudo-randomly generated data matches that in the test code.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qytb8giBXZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_learning_alg(learner, data_gen, n_train, n_test, it):\n",
        "    # Your implementation here\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCZojUBJBb06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_eval_learning_alg(eval_learning_alg, perceptron)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60u9G0QnGzv-",
        "colab_type": "text"
      },
      "source": [
        "## 10.3) Evaluating a Learning Algorithm With a Fixed Dataset\n",
        "\n",
        "Cross-validation is a strategy for evaluating a learning algorithm, using a single training set of size $n$. Cross-validation takes in a learning algorithm $L$, a fixed data set $\\mathcal{D}$, and a parameter $k$. It will run the learning algorithm $k$ different times, then evaluate the accuracy of the resulting classifier, and ultimately return the average of the accuracies over each of the $k$ \"runs\" of $L$. It is structured like this:\n",
        "\n",
        "<pre><code>divide D into k parts, as equally as possible;  call them D_i for i == 0 .. k-1\n",
        "# be sure the data is shuffled in case someone put all the positive examples first in the data!\n",
        "for j from 0 to k-1:\n",
        "    D_minus_j = union of all the datasets D_i, except for D_j\n",
        "    h_j = L(D_minus_j)\n",
        "    score_j = accuracy of h_j measured on D_j\n",
        "return average(score0, ..., score(k-1))</code></pre>\n",
        "\n",
        "So, each time, it trains on  $k−1$ of the pieces of the data set and tests the resulting hypothesis on the piece that was not used for training.\n",
        "\n",
        "When $k=n$, it is called *leave-one-out cross validation*.\n",
        "\n",
        "Implement cross validation **assuming that the input data is shuffled already** so that the positives and negatives are distributed randomly. If the size of the data does not evenly divide by k, split the data into `n % k` sub-arrays of size `n // k + 1` and the rest of size `n // k`. (Hint: You can use <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.array_split.html\">np.array_split</a>\n",
        "and <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html\">np.concatenate</a> with axis arguments to split and rejoin the data as you desire.)\n",
        "\n",
        "Note: In Python, `n//k` indicates integer division, e.g. `2//3 = 0` and `4//3 = 1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5_iixOmBgR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def xval_learning_alg(learner, data, labels, k):\n",
        "    # Cross validation of learning algorithm\n",
        "    # Your implementation here\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUiUgtMHBiZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_xval_learning_alg(xval_learning_alg, perceptron)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crF8flfB3hr1",
        "colab_type": "text"
      },
      "source": [
        "## 11) Testing\n",
        "\n",
        "In this section, we compare the effectiveness of perceptron and averaged perceptron on some data that are not necessarily linearly separable.\n",
        "\n",
        "Use your `eval_learning_alg` and the `gen_flipped_lin_separable`\n",
        "function in the code file to evaluate the accuracy of `perceptron`\n",
        "vs. `averaged_perceptron`.  `gen_flipped_lin_separable` is a wrapper\n",
        "function that returns a generator - `flip_generator`, which can be\n",
        "called with an integer to return a data set and labels.  Note that this\n",
        "generates linearly separable data and then \"flips\" the labels with\n",
        "some specified probability (the argument `pflip`); so most of the\n",
        "results will not be linearly separable.  You can also **specifiy** `pflip`\n",
        "in the call to the generator wrapper function. At the\n",
        "bottom of the code distribution is an example.\n",
        "\n",
        "Run enough trials so that you can confidently predict the accuracy of\n",
        "these algorithms on new data from that same generator; assume\n",
        "training/test sets on the order of 20 points.  The Tutor will check\n",
        "that your answer is within `0.025` of the answer we got using the same\n",
        "generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXFoptqiI6Aw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_gen = gen_flipped_lin_separable(pflip=0.5)\n",
        "print(eval_learning_alg(perceptron, data_gen, n_train=20, n_test=20, it=5))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}