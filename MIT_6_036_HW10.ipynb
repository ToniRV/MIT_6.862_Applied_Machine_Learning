{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIT 6.036 HW10",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ToniRV/MIT_6.862_Applied_Machine_Learning/blob/master/MIT_6_036_HW10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q58cS9antfCw",
        "colab_type": "text"
      },
      "source": [
        "#MIT 6.036 Spring 2020: Homework 10#\n",
        "\n",
        "This colab notebook (<font color=\"red\">version 2020-04-19</font>) provides code and a framework for questions 2, 3, and 4 from [homework 10](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Fall/courseware/Week10/week10_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
        "\n",
        "## <section>**Setup**</section>\n",
        "\n",
        "First, download the code distribution for this homework that contains test cases and helper functions.\n",
        "\n",
        "Run the next code block to download and import the code for this lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUEtSZRdtmI2",
        "colab_type": "code",
        "outputId": "33a7f45f-6587-4174-d639-456e417e2571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "!rm -rf code_for_hw10* __MACOSX data .DS_Store\n",
        "!wget --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/homework/hw10/code_for_hw10.zip\n",
        "!unzip code_for_hw10.zip\n",
        "!mv code_for_hw10/* .\n",
        "\n",
        "import code_for_hw10 as code_for_hw10\n",
        "import mdp10 as mdp\n",
        "\n",
        "import numpy as np\n",
        "import math as m\n",
        "import random\n",
        "\n",
        "import pdb\n",
        "from dist import uniform_dist, delta_dist, mixture_dist, DDist\n",
        "from util import argmax_with_val, argmax\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import importlib"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  code_for_hw10.zip\n",
            "   creating: code_for_hw10/\n",
            "  inflating: code_for_hw10/util.py   \n",
            "  inflating: code_for_hw10/mdp10.py  \n",
            "   creating: code_for_hw10/__pycache__/\n",
            "  inflating: code_for_hw10/__pycache__/util.cpython-37.pyc  \n",
            "  inflating: code_for_hw10/__pycache__/dist.cpython-37.pyc  \n",
            "  inflating: code_for_hw10/code_for_hw10.py  \n",
            "  inflating: code_for_hw10/dist.py   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "       table#id2, #id2 > tbody > tr > th, #id2 > tbody > tr > td {\n",
              "         border: 1px solid lightgray;\n",
              "         border-collapse:collapse;\n",
              "         \n",
              "        }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": [
              "outputarea_id2"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table id=id2><tr><td id=id2-0-0></td><td id=id2-0-1></td><td id=id2-0-2></td><td id=id2-0-3></td><td id=id2-0-4></td><td id=id2-0-5></td><td id=id2-0-6></td><td id=id2-0-7></td><td id=id2-0-8></td><td id=id2-0-9></td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": [
              "outputarea_id2"
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zhptv005XBN",
        "colab_type": "text"
      },
      "source": [
        "# 2) Implement Q-Learning\n",
        "\n",
        "We'll work up to implementing the Q-learning algorithm by extending our code from HW9. If you want, in the next block, copy and paste your implementations of the following functions from HW9. Otherwise, by default we will use the official solutions from HW9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cu9UMTm2l8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def value_iteration(mdp, q, eps = 0.01, max_iters = 1000):\n",
        "    def v(s):\n",
        "        return value(q,s)\n",
        "    for it in range(max_iters):\n",
        "        new_q = q.copy()\n",
        "        delta = 0\n",
        "        for s in mdp.states:\n",
        "            for a in mdp.actions:\n",
        "                new_q.set(s, a, mdp.reward_fn(s, a) + mdp.discount_factor * \\\n",
        "                          mdp.transition_model(s, a).expectation(v))\n",
        "                delta = max(delta, abs(new_q.get(s, a) - q.get(s, a)))\n",
        "        if delta < eps:\n",
        "            return new_q\n",
        "        q = new_q\n",
        "    return q\n",
        "\n",
        "def value(q, s):\n",
        "    return max(q.get(s, a) for a in q.actions)\n",
        "\n",
        "def greedy(q, s):\n",
        "    return argmax(q.actions, lambda a: q.get(s, a))\n",
        "\n",
        "def epsilon_greedy(q, s, eps = 0.5):\n",
        "    if random.random() < eps:  # True with prob eps, random action\n",
        "        return uniform_dist(q.actions).draw()\n",
        "    else:                   # False with prob 1-eps, greedy action\n",
        "        return greedy(q, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyKQPeWk5zx1",
        "colab_type": "text"
      },
      "source": [
        "Run the next code block to make sure what you need from HW9 is working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fvz2c_Vs3JuN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "f5fc3694-91cd-4bb5-ff3d-a5adf6a2e0ce"
      },
      "source": [
        "mdp.value = value\n",
        "mdp.greedy = greedy\n",
        "mdp.epsilon_greedy = epsilon_greedy\n",
        "mdp.value_iteration = value_iteration\n",
        "\n",
        "importlib.reload(code_for_hw10)\n",
        "\n",
        "# Test: Value Iteration\n",
        "code_for_hw10.test_solve_play()\n",
        "\n",
        "# Expected output:\n",
        "# '''\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# '''"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "       table#id4, #id4 > tbody > tr > th, #id4 > tbody > tr > td {\n",
              "         border: 1px solid lightgray;\n",
              "         border-collapse:collapse;\n",
              "         \n",
              "        }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": [
              "outputarea_id4"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table id=id4><tr><td id=id4-0-0></td><td id=id4-0-1></td><td id=id4-0-2></td><td id=id4-0-3></td><td id=id4-0-4></td><td id=id4-0-5></td><td id=id4-0-6></td><td id=id4-0-7></td><td id=id4-0-8></td><td id=id4-0-9></td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": [
              "outputarea_id4"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCaiqNqB6D2-",
        "colab_type": "text"
      },
      "source": [
        "## 2.1) Q update\n",
        "\n",
        "First, we'll extend our implementation of the <tt>TabularQ</tt> class \n",
        "in [HW 9](https://introml.odl.mit.edu/cat-soop/6.036/homework/hw09) \n",
        "(Problem 5) to incorporate the crucial `update` operation of Q-learning, \n",
        "which updates the Q value for a given <tt>(s,a)</tt> entry from its old value, and part of the way towards a \"target\" value $t$:\n",
        "\n",
        "$$Q(s,a) \\leftarrow (1-\\alpha) Q(s,a) + \\alpha t.$$\n",
        "\n",
        "Note that this can also be written as:\n",
        "\n",
        "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha (t - Q(s,a))$$\n",
        "\n",
        "Our `update` method will take in a **batch** of <tt>(s, a, t)</tt> triples and\n",
        "will perform the operation above for all of the triples.  Note that since `update` is a method of the\n",
        "`TabularQ` class, you can access the other methods and attributes of that class.\n",
        "\n",
        "The specifications are:\n",
        "* <code>data</code> is a list of <tt>(s, a, t)</tt> tuples.</li>\n",
        "* <code>lr</code> is a learning rate ($\\alpha$ above)</li>\n",
        "* We will have to update <tt>self.q[(s,a)]</tt> for all of the data.</li>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FZlzaFevNkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TabularQ:\n",
        "    def __init__(self, states, actions):\n",
        "        self.actions = actions\n",
        "        self.states = states\n",
        "        self.q = dict([((s, a), 0.0) for s in states for a in actions])\n",
        "    def copy(self):\n",
        "        q_copy = TabularQ(self.states, self.actions)\n",
        "        q_copy.q.update(self.q)\n",
        "        return q_copy\n",
        "    def set(self, s, a, v):\n",
        "        self.q[(s,a)] = v\n",
        "    def get(self, s, a):\n",
        "        return self.q[(s,a)]\n",
        "    def update(self, data, lr):\n",
        "        # Your code here\n",
        "        for (s, a, t) in data:\n",
        "          new = (1-lr)*self.get(s, a) + lr * t\n",
        "          self.set(s, a, new)\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm5Gb9916E2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_update_method():\n",
        "  q = TabularQ([0,1,2,3],['b','c'])\n",
        "  q.update([(0, 'b', 50), (2, 'c', 30)], 0.5)\n",
        "  q.update([(0, 'b', 25)], 0.5)\n",
        "  if q.get(0, 'b') == 25.0 and q.get(2, 'c') == 15.0: \n",
        "    print('PASSED')\n",
        "    return\n",
        "  print('FAILED')\n",
        "  return\n",
        "\n",
        "# uncomment the line below to test your 'update' method implementation from TabularQ\n",
        "test_update_method()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeQekM6h7vuL",
        "colab_type": "text"
      },
      "source": [
        "## 2.2) Q_learn\n",
        "\n",
        "Recall that in the Q learning algorithm, you need to do two things: \n",
        "<ul>\n",
        "\t<li> simulate the agent's trajectory through the MDP (the world which is being explored) </li>\n",
        "\t<li> update to the Q value estimates for the state-action pairs <tt>(s,a)</tt> encountered on the trajectory\n",
        "</ul>\n",
        "\n",
        "The `Q_learn` function which we will now implement will use the `update` method from the `tabularQ` class to update the Q values, for specific <tt>(s, a)</tt> entries. It will terminate after `iters` iterations and will use learning rate `lr`.\n",
        "\n",
        "In this version, you will do <b>one update after every transition</b>, i.e. you will collect a single <tt>(s, a)</tt> tuple, and update <tt>Q(s, a)</tt>. Remember that the `update` method of `tabularQ` takes in a <tt>(s, a, t)</tt> triple, so you have to add in the target value <tt>t</tt>.\n",
        "\n",
        "\n",
        "**The following methods are available and will be useful:**\n",
        "* To start a new simulation, call `mdp.init_state()` .\n",
        "That will draw a state from the MDP's initial state distribution.\n",
        "* In HW 9 you implemented `epsilon_greedy` for action selection. (`epsilon_greedy` takes `(q, s, eps = 0.5)` as input and returns an action).\n",
        "* In HW 9 you implemented `value` which takes `(q, s)` as input and returns the max Q value for a state.\n",
        "* To take a step in the simulation (MDP), starting in a given state <tt>s</tt>, using action <tt>a</tt>, call\n",
        "`mdp.sim_transition(s,a)`.  It will return a pair <tt>(r, s_prime)</tt> denoting the reward received by the agent as well as the next state.\n",
        "* `mdp.discount_factor` reveals the discount factor being used.\n",
        "\n",
        "**Note about terminal states:** \n",
        "* Recall that at a terminal state, there may be an immediate reward (i.e. allow for reward <tt>r</tt>) but the future expected value will be zero. (You can achieve that with $\\gamma=0$ or simply dropping the term with $\\gamma$).\n",
        "* Also, when reaching a terminal state. Simulations will automaticall y restart with a new initial state if needed, so you do *not* need any terminal condition and you do *not* need to call `mdp.init_state()`).\n",
        "\n",
        "\n",
        "**Your code should return return `q`** so that the Tutor can test it. Locally, or in colab, you should be able to pass `draw=True` in the test function `test_learn_play` to see a policy learned by the algorithm playing the game No Exit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00YIgjEwvOab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Q_learn(mdp, q, lr=.1, iters=100, eps = 0.5, interactive_fn=None):\n",
        "    # Your code here\n",
        "    for i in range(iters):\n",
        "        # Your code here\n",
        "        # include this line in the iteration, where i is the iteration number\n",
        "        if interactive_fn: interactive_fn(q, i)\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UweE9iUL897r",
        "colab_type": "text"
      },
      "source": [
        "Run the next code blocks to test your implementation of `Q_learn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rku6GFpdgMdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mdp.TabularQ = TabularQ\n",
        "mdp.Q_learn = Q_learn\n",
        "importlib.reload(code_for_hw10)\n",
        "\n",
        "# Test: Tabular Q-learn\n",
        "code_for_hw10.test_learn_play(iters=100000, tabular=True, batch=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzEz4q7y3R8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tinyTerminal(s):\n",
        "    return s==4\n",
        "def tinyR(s, a):\n",
        "    if s == 1: return 1\n",
        "    elif s == 3: return 2\n",
        "    else: return 0\n",
        "def tinyTrans(s, a):\n",
        "    if s == 0:\n",
        "        if a == 'a':\n",
        "            return DDist({1 : 0.9, 2 : 0.1})\n",
        "        else:\n",
        "            return DDist({1 : 0.1, 2 : 0.9})\n",
        "    elif s == 1:\n",
        "        return DDist({1 : 0.1, 0 : 0.9})\n",
        "    elif s == 2:\n",
        "        return DDist({2 : 0.1, 3 : 0.9})\n",
        "    elif s == 3:\n",
        "        return DDist({3 : 0.1, 0 : 0.5, 4 : 0.4})\n",
        "    elif s == 4:\n",
        "        return DDist({4 : 1.0})\n",
        "      \n",
        "def testQ():\n",
        "    tiny = mdp.MDP([0, 1, 2, 3, 4], ['a', 'b'], tinyTrans, tinyR, 0.9)\n",
        "    tiny.terminal = tinyTerminal\n",
        "    q = TabularQ(tiny.states, tiny.actions)\n",
        "    qf = Q_learn(tiny, q)\n",
        "    ret = list(qf.q.items())\n",
        "    expected = [((0, 'a'), 0.6649739221724159), ((0, 'b'), 0.1712369526453748), \n",
        "                ((1, 'a'), 0.7732751316011999), ((1, 'b'), 1.2034912054227331), \n",
        "                ((2, 'a'), 0.37197205380133874), ((2, 'b'), 0.45929063274463033), \n",
        "                ((3, 'a'), 1.5156163024818292), ((3, 'b'), 0.8776852768653631), \n",
        "                ((4, 'a'), 0.0), ((4, 'b'), 0.0)]\n",
        "    ok = True\n",
        "    for (s,a), v in expected:\n",
        "      qv = qf.get(s,a)\n",
        "      if abs(qv-v) > 1.0e-5:\n",
        "        print(\"Oops!  For (s=%s, a=%s) expected %s, but got %s\" % (s, a, v, qv))\n",
        "        ok = False\n",
        "    if ok:\n",
        "      print(\"Tests passed!\")\n",
        "\n",
        "random.seed(0)\n",
        "testQ()      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PXWcgn99NfK",
        "colab_type": "text"
      },
      "source": [
        "## 2.3) Batch Q_learn\n",
        "\n",
        "In `Q_learn`, we made **one transition** <tt>(s,a,r,s')</tt>, computed a target Q value $t$ and updated the corresponding <tt>Q(s,a)</tt> estimate, every iteration. \n",
        "\n",
        "Now, we will implement `Q_learn_batch` which in each iteration, will generate **several such transitions** and update several <tt>Q(s,a)</tt> estimates correspondignly. \n",
        "\n",
        "Let's define an \"episode\" in the MDP as a full cycle of transitions, starting from first randomly picked start state $s_0$ to the terminal state which ends that \"episode\" of exploration. For example, in an episode of length 3, we perform 3 actions, yielding the following sequence of tuples: <tt>[(s_0,a_0,r_0,s_1), (s_1,a_1,r_1,s_2), (s_2,a_2,r_2,s_3)]</tt>.\n",
        "\n",
        "So concretely, in every iteration of `Q_learn_batch`, we generate `n_episodes` new \"episodes\" using the current Q values for picking actions through our `epsilon_greedy` method. We then perform a batched update of the Q\n",
        "values based on all the accumulated <tt>(s,a,r,s')</tt> tuples, and their corresponding target <tt>t</tt> values.\n",
        "\n",
        "One last detail about our implementation is that in each iteration, **we will also use <tt>(s,a,r,s')</tt> tuples gathered from previous iterations to update Q values**. Notice that regardless of the iteration in which a <tt>(s,a,r,s')</tt> tuple was generated, it will always be representative of the dynamics of the MDP which we are exploring. So these tuples can be re-used across iterations of `Q_learn_batch` to make each iteration's Q value updates more informative.\n",
        "\n",
        "**Note though,** that as Q values change every iteration, the target Q value <tt>t</tt> which goes with a certain <tt>(s,a,r,s')</tt> tuple in one iteration, will probably need to change when that same <tt>(s,a,r,s')</tt> tuple is re-used in another iteration. Make sure you understand why.\n",
        "\n",
        "Here's pseudocode for what `Q_learn_batch` should be doing:\n",
        "\n",
        "<pre>\n",
        "# all collected (s,a,r,s') tuples across all iterations, are accumulated in all_experiences\n",
        "all_experiences = []\n",
        "\n",
        "Loop over n_iterations of Q-learn_batch:\n",
        "    \n",
        "    ''' data generation '''\n",
        "    for i in range(n_episodes):\n",
        "        Generate an episode (a sequence of (s,a,r,s') tuples) of length episode_length\n",
        "\tAdd all experience (s,a,r,s') tuples from this episode to all_experiences list\n",
        "    \n",
        "    ''' update Q values - compute fresh targets for EVERY experience tuple'''\n",
        "    all_q_targets = []\n",
        "    \n",
        "    For each (s,a,r,s') tuple from all_experiences list:\n",
        "        Append (s,a,t) tuple to all_q_targets list, where t is the new Q target\n",
        "        Remember to handle terminal states (where s' = None)\n",
        "    \n",
        "    q.update(all_q_targets, lr)\n",
        "\n",
        "# return q so that the Tutor can test it\n",
        "return q\n",
        "</pre>\n",
        "\n",
        "\n",
        "**The `sim_episode` method provided  will be useful and you can find the method definition in the next code block below:**\n",
        "\n",
        "* `sim_episode` takes in an `mdp`, `episode_length` a `policy`, and a boolean `draw` as inputs, and it simulates an episode (sequence of transitions) in the provided `mdp`, of at most `episode_length`, using the `policy` function to select actions. If it finds a terminal state, it ends the episode.\n",
        "* It returns the accumulated `reward` followed by an `episode` i.e. a list of <tt>(s, a, r, s')</tt> tuples, where s' is `None` for any transition from a terminal state.\n",
        "* If `draw=True` is provided to `sim_episode`, its last returned object will be an animation, otherwise it will simply be `None`.\n",
        "\n",
        "**Note that you should return `q`** so that the Tutor can test it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WMcWSfnE-_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate this cell so you can use the definition in your code below\n",
        "\n",
        "def sim_episode(mdp, episode_length, policy, draw=False):\n",
        "    '''\n",
        "    Simulate an episode (sequence of transitions) of at most\n",
        "    episode_length, using policy function to select actions.  If we find\n",
        "    a terminal state, end the episode.  Return accumulated reward a list\n",
        "    of (s, a, r, s') where s' is None for transition from terminal state.\n",
        "    Also return an animation if draw=True, or None if draw=False\n",
        "    '''\n",
        "    episode = []\n",
        "    reward = 0\n",
        "    s = mdp.init_state()\n",
        "    all_states = [s]\n",
        "    for i in range(episode_length):\n",
        "        a = policy(s)\n",
        "        (r, s_prime) = mdp.sim_transition(s, a)\n",
        "        reward += r\n",
        "        if mdp.terminal(s):\n",
        "            episode.append((s, a, r, None))\n",
        "            break\n",
        "        episode.append((s, a, r, s_prime))\n",
        "        if draw: \n",
        "            mdp.draw_state(s)\n",
        "        s = s_prime\n",
        "        all_states.append(s)\n",
        "    animation = animate(all_states, mdp.n, episode_length) if draw else None\n",
        "    return reward, episode, animation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uHjoRWPvScc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Q_learn_batch(mdp, q, lr=.1, iters=100, eps=0.5,\n",
        "                  episode_length=10, n_episodes=2,\n",
        "                  interactive_fn=None):\n",
        "    # Your code here\n",
        "    for i in range(iters):\n",
        "        # Your code also here\n",
        "        # include this line in the iteration, where i is the iteration number\n",
        "        if interactive_fn: interactive_fn(q, i)\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv9oygbu9Sz1",
        "colab_type": "text"
      },
      "source": [
        "Run the next code blocks to test your implementation of `Q_learn_batch`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0FnwKY5gqNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mdp.Q_learn_batch = Q_learn_batch\n",
        "importlib.reload(code_for_hw10)\n",
        "\n",
        "# Test: Tabular Batch Q-learn\n",
        "code_for_hw10.test_learn_play(iters=10, tabular=True, batch=True) # Check: why do we want fewer iterations here?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFvMhZuL3-Y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testBatchQ():\n",
        "    tiny = mdp.MDP([0, 1, 2, 3, 4], ['a', 'b'], tinyTrans, tinyR, 0.9)\n",
        "    tiny.terminal = tinyTerminal\n",
        "    q = TabularQ(tiny.states, tiny.actions)\n",
        "    qf = Q_learn_batch(tiny, q)\n",
        "    ret = list(qf.q.items())\n",
        "    expected = [((0, 'a'), 4.7566600197286535), ((0, 'b'), 3.993296047838986), \n",
        "                ((1, 'a'), 5.292467934685342), ((1, 'b'), 5.364014782870985), \n",
        "                ((2, 'a'), 4.139537149779127), ((2, 'b'), 4.155347555640753), \n",
        "                ((3, 'a'), 4.076532544818926), ((3, 'b'), 4.551442974149778), \n",
        "                ((4, 'a'), 0.0), ((4, 'b'), 0.0)]\n",
        "\n",
        "    ok = True\n",
        "    for (s,a), v in expected:\n",
        "      qv = qf.get(s,a)\n",
        "      if abs(qv-v) > 1.0e-5:\n",
        "        print(\"Oops!  For (s=%s, a=%s) expected %s, but got %s\" % (s, a, v, qv))\n",
        "        ok = False\n",
        "    if ok:\n",
        "      print(\"Tests passed!\")\n",
        "      \n",
        "      return list(qf.q.items())\n",
        "\n",
        "random.seed(0)\n",
        "testBatchQ()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj5q0rup9eq0",
        "colab_type": "text"
      },
      "source": [
        "# 3) NN Q: Using neural networks to store the Q function\n",
        "\n",
        "We would like to operate in large or continuous state and/or action\n",
        "spaces so it is not possible (or effective) to store the $Q$ values in\n",
        "a table as we did with the <tt>TabularQ</tt> class; instead, we will\n",
        "\"store\" them by training a neural network to do regression for us,\n",
        "taking $s,a$ as input and generating (an approximation of) $Q^*(s,a)$\n",
        "as output.\n",
        "\n",
        "To train the network, we will use <i>squared Bellman error</i> as\n",
        "the loss function:\n",
        "$$\\left(\\left[R(s_t, a_t) + \\gamma \\max_{a'} Q(s_{t+1}, a'; \\theta)\\right]\n",
        "- Q(s_t, a_t;\\theta) \\right)^2$$\n",
        "where $\\theta$ stands for the current weights in the neural network\n",
        "and $Q(s, a; \\theta)$ stands for the output of the network with\n",
        "weights $\\theta$ when $(s,a)$ is the input.\n",
        "\n",
        "There are many choices of neural network architecture for storing Q\n",
        "values.  In this problem, we will:\n",
        "\n",
        "<ul>\n",
        "\n",
        "<li> Focus on the case where we have a small set of possible actions,\n",
        "so make one neural network for each possible action <math>a</math>;\n",
        "\n",
        "<li> Design that network with two <b>hidden</b> layers with ReLU units\n",
        "and a single linear output unit (although a deeper network could be\n",
        "useful); and\n",
        "\n",
        "<li> Use mean squared error (MSE) as the loss function since, we are\n",
        "predicting continuous <math>Q</math> values, which is a regression\n",
        "problem.\n",
        "\n",
        "</ul>\n",
        "\n",
        "To use a neural net to store Q values, for a given action, we will\n",
        "need to have a mapping from states to fixed-length vectors.  We will\n",
        "assume that the <code>MDP</code> class has a <code>state2vec</code>\n",
        "method that maps states to vectors.  For the simple discrete-state\n",
        "MDPs we have seen so far, this simply returns a one-hot representation\n",
        "of the state. \n",
        "\n",
        "For reference, this is our implementation of\n",
        "<code>state2vec</code> (note the shape of its returned array):\n",
        "<pre>\n",
        "    def state2vec(self, s):\n",
        "        '''\n",
        "        Return one-hot encoding of state s; used in neural network agent implementations\n",
        "        '''\n",
        "        v = np.zeros((1, len(self.states)))\n",
        "        v[0,self.states.index(s)] = 1.\n",
        "        return v\n",
        "</pre>\n",
        "\n",
        "Now, all we need to do is write a new class, called <code>NNQ</code>\n",
        "to implement neural-network version of Q-function storage; then we can\n",
        "pass an <code>NNQ</code> instance instead of a <code>TabularQ</code> instance\n",
        "into <code>Q_learn</code> or <code>Q_learn_batch</code>, and\n",
        "we will automatically have reinforcement learning with neural\n",
        "networks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFEt8UXOR_v7",
        "colab_type": "text"
      },
      "source": [
        "There are three methods to implement in our <code>NNQ</code>\n",
        "class. Here are some ideas for how to do that:\n",
        "<ul>\n",
        "\n",
        "<li> <code>__init__</code>: Create one neural network for each action,\n",
        "and store them in <tt>self.models</tt>. Note that <tt>actions</tt> is\n",
        "a list that may consist of integers or strings or other objects.  As a\n",
        "reminder, here's how to make a new feed-forward network using PyTorch:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAhjpjZ-CoD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# please evaluate this cell so you can use it in your code\n",
        "import torch\n",
        "from torch import nn\n",
        "def make_nn(state_dim, num_hidden_layers, num_units):\n",
        "    print ('Making network', state_dim, num_hidden_layers, num_units)\n",
        "    '''\n",
        "\n",
        "    state_dim =\t(int) number of states\n",
        "    num_hidden_layers =\t(int) number of\tfully connected hidden layers\n",
        "    num_units =\t(int) number of\tdense relu units to use\tin hidden layers\n",
        "    '''  \n",
        "    model = []\n",
        "    model += [nn.Linear(state_dim, num_units), nn.ReLU()]\n",
        "    for i in range(num_hidden_layers-1):\n",
        "        model += [nn.Linear(num_units, num_units), nn.ReLU()]\n",
        "    model += [nn.Linear(num_units, 1)]\n",
        "    model = nn.Sequential(*model)\n",
        "    def init_weights(w):\n",
        "      if type(w) == nn.Linear: nn.init.xavier_normal_(w.weight)\n",
        "    print('Model: ', model)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozZOU_BnR4Tl",
        "colab_type": "text"
      },
      "source": [
        "<ul>\n",
        "<li> <code>get(self, s, a)</code>: Use the neural network you have\n",
        "stored for action <code>a</code> to predict a Q value for state\n",
        "<code>s</code>. Feel free to consult a list of relevant\n",
        "<a href=\"https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-custom-nn-modules\">PyTorch code examples</a>\n",
        ".</li>\n",
        "\n",
        "<li> <code>update(self, data, lr, epochs = 1)</code>: As in\n",
        "<code>TabularQ</code>, <code>data</code> is a list of <code>(s, a,t)</code>\n",
        "tuples, where <code>t</code> is a target Q value.  For each\n",
        "action <code>a</code>, you will need to:\n",
        "\n",
        "  <ul>\n",
        "  <li> Construct a training set <code>X, Y</code> of data that is\n",
        "  relevant to action <code>a</code>, where the input values are states\n",
        "  (encoded as vectors) and the output values are the target Q values;</li>\n",
        "\n",
        "  <li> Use the provided method <code>fit(self, model, X, Y, epochs=epochs)</code>\n",
        "  to update the weights in the associated network. </li>\n",
        "  </ul>\n",
        "</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKE6GBaxveRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Complete the following definition to implement the NNQ class\n",
        "class NNQ:\n",
        "    def __init__(self, states, actions, state2vec, num_layers, num_units,\n",
        "                 lr=1e-2, epochs=1):\n",
        "        self.running_loss = 0. # To keep a running average of the loss\n",
        "        self.running_one = 0. # idem\n",
        "        self.num_running = 0.001 # idem\n",
        "        self.lr = lr\n",
        "        self.actions = actions\n",
        "        self.states = states\n",
        "        self.state2vec = state2vec\n",
        "        self.epochs = epochs\n",
        "        state_dim = state2vec(states[0]).shape[1] # a row vector\n",
        "        self.models = None        # Your code here\n",
        "    def predict(self, model, s):\n",
        "      return model(torch.FloatTensor(self.state2vec(s))).detach().numpy()\n",
        "    def get(self, s, a):\n",
        "        # Your code here\n",
        "        pass\n",
        "    def fit(self, model, X,Y, epochs=None, dbg=None):\n",
        "      # This function receives two numpy arrays (with shape (K,7) and (K,1)), not two lists!\n",
        "      assert type(X) is not type([]), \"self.fit receives two numpy arrays (with shape (K,7) and (K,1)), not two lists!\"\n",
        "      if epochs is None: epochs = self.epochs\n",
        "      train = torch.utils.data.TensorDataset(torch.FloatTensor(X), torch.FloatTensor(Y))\n",
        "      train_loader = torch.utils.data.DataLoader(train, batch_size=256,shuffle=True)\n",
        "      opt = torch.optim.SGD(model.parameters(), lr=self.lr)\n",
        "      for epoch in range(epochs):\n",
        "        for (X,Y) in train_loader:\n",
        "          opt.zero_grad()\n",
        "          loss = torch.nn.MSELoss()(model(X), Y)\n",
        "          loss.backward()\n",
        "          self.running_loss = self.running_loss*(1.-self.num_running) + loss.item()*self.num_running\n",
        "          self.running_one = self.running_one*(1.-self.num_running) + self.num_running\n",
        "          opt.step()\n",
        "      if dbg is True or (dbg is None and np.random.rand()< (0.001*X.shape[0])):\n",
        "        print('Loss running average: ', self.running_loss/self.running_one)\n",
        "\n",
        "    def update(self, data, lr, dbg=None):\n",
        "        # Your code here: train the model for every action\n",
        "        # Remember to check there is actually data to train on!\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ1PYVqe9Vq0",
        "colab_type": "text"
      },
      "source": [
        "\n",
        " \n",
        "Run the next code blocks to test your implementation of `NNQ`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYNYWo7BvfIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_NNQ(data):\n",
        "    tiny = mdp.MDP([0, 1, 2, 3, 4], ['a', 'b'], tinyTrans, tinyR, 0.9)\n",
        "    tiny.terminal = tinyTerminal\n",
        "    q = NNQ(tiny.states, tiny.actions, tiny.state2vec, 2, 10)\n",
        "    q.update(data, 1)\n",
        "    ret =  [q.get(s,a) for s in q.states for a in q.actions]\n",
        "    expect = [np.array([[-0.07211456]]), np.array([[-0.19553234]]), \n",
        "              np.array([[-0.21926211]]), np.array([[0.01699455]]), \n",
        "              np.array([[-0.26390356]]), np.array([[0.06374809]]), \n",
        "              np.array([[0.0340214]]), np.array([[-0.18334733]]), \n",
        "              np.array([[-0.438375]]), np.array([[-0.13844737]])]\n",
        "    cnt = 0\n",
        "    ok = True\n",
        "    for s in q.states:\n",
        "      for a in q.actions:\n",
        "        if not np.all(np.abs(ret[cnt]-expect[cnt]) < 1.0e0):\n",
        "          print(\"Oops, for s=%s, a=%s expected %s but got %s\" % (s, a, expect[cnt], ret[cnt]))\n",
        "          ok = False\n",
        "        cnt += 1\n",
        "    if ok:\n",
        "      print(\"Output looks generally ok\")\n",
        "    return q\n",
        "  \n",
        "test_NNQ([(0,'a',0.3),(1,'a',0.1),(0,'a',0.1),(1,'a',0.5)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KG2el3iYHsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tinyTrans2(s, a):\n",
        "    if s == 0:\n",
        "        return DDist({1 : 1.0})\n",
        "    elif s == 1:\n",
        "        return DDist({2 : 1.0})\n",
        "    elif s == 2:\n",
        "        return DDist({3 : 1.0})\n",
        "    elif s == 3:\n",
        "        return DDist({4 : 1.0})\n",
        "    elif s == 4:\n",
        "        return DDist({4 : 1.0})\n",
        "\n",
        "def test_NNQ2(data):\n",
        "    np.random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    tiny = mdp.MDP([0, 1, 2, 3, 4], ['a', 'b'], tinyTrans2, tinyR, 0.9)\n",
        "    tiny.terminal = tinyTerminal\n",
        "    q = NNQ(tiny.states, tiny.actions, tiny.state2vec, 2, 10)\n",
        "    q.update(data, 1)\n",
        "    return [q.get(s,a).item(0) for s in q.states for a in q.actions]\n",
        "  \n",
        "print(test_NNQ2([(0,'a',0.3),(1,'a',0.1),(0,'a',0.1),(1,'a',0.5)]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB9pR7h0R-0H",
        "colab_type": "text"
      },
      "source": [
        " **The unbatched version of `NNQ` is pretty unstable, you may thus have to lower the learning rate of the optimizer.** To decide whether to change the learning rate, observe if the loss diverges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxQqOP7Qg2-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mdp.NNQ = NNQ\n",
        "importlib.reload(code_for_hw10)\n",
        "\n",
        "# Test: NN Q-learn\n",
        "code_for_hw10.test_learn_play(nnq_lr=1e-2, iters=100000, tabular=False, batch=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quq-Uh44_Bbo",
        "colab_type": "text"
      },
      "source": [
        "## 3.2) Fitted Q iteration\n",
        "\n",
        "*Fitted Q iteration (FQ)* suffers less from the correlated experience problem and is generally more stable (and sometimes slower) than NNQ. **You can therefore use a 2-10x higher learning rate than the unbatched version, which should speed results.**\n",
        "\n",
        "FQ initializes the Q networks and an empty data set, then operates in a loop:\n",
        "\n",
        "1. Use *ϵ*-greedy exploration to generate *k* steps of experience, of the form *(s,a,r,s′)* and add them to the data set.\n",
        "2. Create one training set for each action *a*:\n",
        "\n",
        "> 1. Extract all the tuples from your data set that contain action *a*,\n",
        "> 2. Let the *X* values of your training set be all of the *s* values from your data tuples with action *a* and the *Y* values be the *r + γ max_a' Q(s', a')* values computed for each data tuple, using the Q estimates from the current network.\n",
        "\n",
        "3. Train the network for action *a* for several epochs until it has done a good job of representing this data.\n",
        "\n",
        "So, this is basically `Q_learn_batch` using `NNQ` (training with multiple epochs) to implement the Q function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kW_URSm9mOI",
        "colab_type": "text"
      },
      "source": [
        "Run the next code block to test your implementation of `NNQ` with batching."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY6H72oV4z5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test: NN Batch Q-learn (Fitted Q-learn)\n",
        "code_for_hw10.test_learn_play(nnq_lr=1e-2, iters=50, tabular=False, batch=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d4QW3g0ww5H",
        "colab_type": "text"
      },
      "source": [
        "#4) No Exit\n",
        "\n",
        "Please read the instructions in the [homework](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week10/week10_homework/) to set up for the game. You may use this space to display the game in Colab.\n",
        "\n",
        "For each of the learning method and Q model combinations below,\n",
        "solve the game so that it reliably gets to reward of 100 (that is, the\n",
        "learned game reliably plays 100 steps without missing the ball,\n",
        "earning a score of 100).  During learning, you should see a sequence\n",
        "of lines like: <code>score (5000, 37.5)</code>, which indicates that\n",
        "after 5000 iterations the average reward over 10 games is 37.5.  We\n",
        "are checking whether you reach a solution that gets an average reward\n",
        "100 at least one time. Try playing around with the number of\n",
        "iterations (an argument to <code>test_learn_play</code>) until you\n",
        "achieve this point. Note that we will need fewer iterations for\n",
        "Q_learn_batch, in general (check yourself: why?). After learning, the\n",
        "code prints a long \"upload string\" in HEX code.  Enter the upload\n",
        "strings in the question boxes in the homework MITx site."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRbeO95qxJf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Value Iteration\n",
        "code_for_hw10.test_solve_play(draw = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXA-F66nWizC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tabular Q-learn\n",
        "code_for_hw10.test_learn_play(draw=True, iters=100000, tabular=True, batch=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xWEvfLiWmdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tabular Batch Q-learn\n",
        "code_for_hw10.test_learn_play(draw=True, iters=50, tabular=True, batch=True) # Check: why do we want fewer iterations here?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtiEnPOZWrNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NN Q-learn\n",
        "code_for_hw10.test_learn_play(draw=True, iters=100000, tabular=False, batch=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqGjYEq5WuKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NN Batch Q-learn (Fitted Q-learn)\n",
        "code_for_hw10.test_learn_play(draw=True, iters=50, tabular=False, batch=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}