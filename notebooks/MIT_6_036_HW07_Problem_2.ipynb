{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIT 6.036 HW07 Problem 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ToniRV/MIT_6.862_Applied_Machine_Learning/blob/master/MIT_6_036_HW07_Problem_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xIaEwCD406A",
        "colab_type": "text"
      },
      "source": [
        "#MIT 6.036 Spring 2020: Homework 7 Problem 2#\n",
        "\n",
        "This colab notebook provides code and a framework for problem 2 of [the homework](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week7/week7_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
        "\n",
        "**Note**: You can go to `File > Save a copy in Drive...` to save your own copy of this notebook for editing.\n",
        "\n",
        "## <section>**Setup**</section>\n",
        "\n",
        "First, download the code distribution for this homework that contains test cases and helper functions.\n",
        "\n",
        "Run the next code block to download and import the code for this lab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YM-_zLf9Bp-",
        "colab_type": "code",
        "outputId": "02063d6d-3f0a-414d-b1fd-b116be5a78ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!rm -rf code_for_hw7*\n",
        "!rm -rf mnist\n",
        "!wget --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/homework/hw07/code_for_hw7.zip\n",
        "!unzip code_for_hw7.zip\n",
        "!mv code_for_hw7/* .\n",
        "\n",
        "from code_for_hw7 import *\n",
        "import numpy as np\n",
        "import modules_disp as disp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  code_for_hw7.zip\n",
            "   creating: code_for_hw7/\n",
            "  inflating: code_for_hw7/utils_hw7.py  \n",
            "  inflating: code_for_hw7/code_for_hw7_pytorch.py  \n",
            "  inflating: code_for_hw7/code_for_hw7.py  \n",
            "  inflating: code_for_hw7/modules_disp.py  \n",
            "   creating: code_for_hw7/data/\n",
            "  inflating: code_for_hw7/data/data2_validate.csv  \n",
            "  inflating: code_for_hw7/data/dataXor_train.csv  \n",
            "  inflating: code_for_hw7/data/data1_train.csv  \n",
            "  inflating: code_for_hw7/data/data4_train.csv  \n",
            "  inflating: code_for_hw7/data/data4_validate.csv  \n",
            "  inflating: code_for_hw7/data/data3class_train.csv  \n",
            "  inflating: code_for_hw7/data/data1_validate.csv  \n",
            "  inflating: code_for_hw7/data/data3_train.csv  \n",
            "  inflating: code_for_hw7/data/data2_train.csv  \n",
            "  inflating: code_for_hw7/data/data3_validate.csv  \n",
            "  inflating: code_for_hw7/expected_results.py  \n",
            "mv: cannot move 'code_for_hw7/data' to './data': Directory not empty\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFxhrJ5XDlvb",
        "colab_type": "text"
      },
      "source": [
        "# 2) Implementing Neural Networks\n",
        "\n",
        "This homework considers neural networks with multiple layers. Each layer has multiple inputs and outputs, and can be broken down into two parts:\n",
        "\n",
        "<br>\n",
        "A linear module that implements a linear transformation:     $ z_j = (\\sum^{m}_{i=1} x_i W_{i,j}) + {W_0}_jz$\n",
        "\n",
        "specified by a weight matrix $W$ and a bias vector $W_0$. The input is $[x_1, \\ldots, x_m]^T$. The output is $[z_1, \\ldots, z_n]^T$\n",
        "\n",
        "<br>\n",
        "An activation module that applies an activation function to the outputs of the linear module for some activation function $f$, such as Tanh or ReLU in the hidden layers or Softmax (see below) at the output layer. We write the output as: $[f(z_1), \\ldots, f(z_m)]^T$, although technically, for some activation functions such as softmax, each output will depend on all the $z_i$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjQgtwPHj08n",
        "colab_type": "text"
      },
      "source": [
        "Although for \"real\" applications you want to use one of the many packaged implementations of neural networks (we'll start using one of those soon), there is no substitute for implementing one yourself to get an in-depth understanding. Luckily, that is relatively easy to do if we're not too concerned with maximum efficiency.\n",
        "\n",
        "We'll use the modular implementation that we guided you through in the previous problem, which leads to clean code. The basic framework for SGD training is given below. We can construct a network and train it as follows:\n",
        "\n",
        "```\n",
        "# build a 3-layer network\n",
        "net = Sequential([Linear(2,3), Tanh(),\n",
        "                  Linear(3,3), Tanh(),\n",
        "    \t          Linear(3,2), SoftMax()])\n",
        "# train the network on data and labels\n",
        "net.sgd(X, Y)\n",
        "```\n",
        "We will (later) be generalizing SGD to operate on a \"mini-batch\" of data points instead of a single point. You should strive for an implementation of the forward, backward, and `class_fun` methods that works with batches of data. Note that when $b$ is mentioned as part of the shape of a matrix in the code, this $b$ refers to the number of points.\n",
        "\n",
        "Please fill in any unimplemented methods below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEwpgsbnho9K",
        "colab_type": "text"
      },
      "source": [
        "## Linear Modules: ##\n",
        "Each linear module has a forward method that takes in a batch of activations A (from the previous layer) and returns a batch of pre-activations Z.\n",
        "\n",
        "Each linear module has a backward method that takes in dLdZ and returns dLdA. This module also computes and stores dLdW and dLdW0, the gradients with respect to the weights.\n",
        "\n",
        "Hint: be careful with dimensions when computing dLdW0. dLdZ is (n x b), but dLdW0 is (n x 1). Why do you need to sum over all $b$ points in the batch when computing dLdW0?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VsYLAxCfy7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Linear(Module):\n",
        "    def __init__(self, m, n):\n",
        "        self.m, self.n = (m, n)  # (in size, out size)\n",
        "        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
        "        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n",
        "\n",
        "    def forward(self, A):\n",
        "        self.A = A   # (m x b)  Hint: make sure you understand what b stands for\n",
        "        return None  # Your code (n x b)\n",
        "\n",
        "    def backward(self, dLdZ):  # dLdZ is (n x b), uses stored self.A\n",
        "        self.dLdW = None       # Your code\n",
        "        self.dLdW0 = None      # Your code\n",
        "        return None            # Your code: return dLdA (m x b)\n",
        "\n",
        "    def sgd_step(self, lrate):  # Gradient descent step\n",
        "        self.W = None           # Your code\n",
        "        self.W0 = None          # Your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqZ7_kZYr5s5",
        "colab_type": "text"
      },
      "source": [
        " You are encouraged to make your own tests for each module. A unit test method and an example test case are given below for your reference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY3yePY0r4eA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "# data\n",
        "X, Y = super_simple_separable()\n",
        "\n",
        "# module\n",
        "linear_1 = Linear(2, 3)\n",
        "\n",
        "#hyperparameters\n",
        "lrate = 0.005\n",
        "\n",
        "# test case\n",
        "# forward\n",
        "z_1 = linear_1.forward(X)\n",
        "exp_z_1 =  np.array([[10.41750064, 6.91122168, 20.73366505, 22.8912344],\n",
        "                     [7.16872235, 3.48998746, 10.46996239, 9.9982611],\n",
        "                     [-2.07105455, 0.69413716, 2.08241149, 4.84966811]])\n",
        "unit_test(\"linear_forward\", exp_z_1, z_1)\n",
        "\n",
        "# backward\n",
        "dL_dz1 = np.array([[1.69467553e-09, -1.33530535e-06, 0.00000000e+00, -0.00000000e+00],\n",
        "                                     [-5.24547376e-07, 5.82459519e-04, -3.84805202e-10, 1.47943038e-09],\n",
        "                                     [-3.47063705e-02, 2.55611604e-01, -1.83538094e-02, 1.11838432e-04]])\n",
        "exp_dLdX = np.array([[-2.40194628e-02, 1.77064845e-01, -1.27021626e-02, 7.74006953e-05],\n",
        "                                    [2.39827939e-02, -1.75870737e-01, 1.26832126e-02, -7.72828555e-05]])\n",
        "dLdX = linear_1.backward(dL_dz1)\n",
        "unit_test(\"linear_backward\", exp_dLdX, dLdX)\n",
        "\n",
        "# sgd step\n",
        "linear_1.sgd_step(lrate)\n",
        "exp_linear_1_W = np.array([[1.2473734,  0.28294514,  0.68940437],\n",
        "                           [1.58455079, 1.32055711, -0.69218045]]),\n",
        "unit_test(\"linear_sgd_step_W\",  exp_linear_1_W,  linear_1.W)\n",
        "\n",
        "exp_linear_1_W0 = np.array([[6.66805339e-09],\n",
        "                            [-2.90968033e-06],\n",
        "                            [-1.01331631e-03]]),\n",
        "unit_test(\"linear_sgd_step_W0\", exp_linear_1_W0, linear_1.W0)\n",
        "\n",
        "# forward with non-zero bias\n",
        "np.random.seed(0)\n",
        "linear_2 = Linear(2, 3)\n",
        "linear_2.W0 = np.array([[1],[1],[1]]) #create new network with nonzero bias\n",
        "z_2 = linear_2.forward(X)\n",
        "expz_2 = np.array([[11.41750064,  7.91122168, 21.73366505, 23.8912344 ],\n",
        "                   [ 8.16872235,  4.48998746, 11.46996239, 10.9982611 ],\n",
        "                   [-1.07105455,  1.69413716,  3.08241149,  5.84966811]])\n",
        "unit_test(\"linear_forward_nonzero_bias\", expz_2, z_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ETL01mPsBz4",
        "colab_type": "text"
      },
      "source": [
        "The following datasets are defined for your use:\n",
        "*  `super_simple_separable_through_origin()`\n",
        "*  `super_simple_separable()`\n",
        "*  `xor()`\n",
        "*  `xor_more()`\n",
        "*  `hard()`\n",
        "\n",
        "Further, a plotting function is defined for your usage in modules_disp.py, and can be called in the colab notebook as `disp.plot_nn()`.\n",
        "```\n",
        "def plot_nn(X, Y, nn):\n",
        "    \"\"\" Plot output of nn vs. data \"\"\"\n",
        "    def predict(x):\n",
        "        return nn.modules[-1].class_fun(nn.forward(x))[0]\n",
        "    xmin, ymin = np.min(X, axis=1)-1\n",
        "    xmax, ymax = np.max(X, axis=1)+1\n",
        "    nax = plot_objective_2d(lambda x: predict(x), xmin, xmax, ymin, ymax)\n",
        "    plot_data(X, Y, nax)\n",
        "    plt.show()```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s70beWJh09h",
        "colab_type": "text"
      },
      "source": [
        "## Activation functions: ##\n",
        "Each activation module has a forward method that takes in a batch of pre-activations Z and returns a batch of activations A.\n",
        "\n",
        "Each activation module has a backward method that takes in dLdA and returns dLdZ, with the exception of SoftMax, where we assume dLdZ is passed in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwaNAtLnhenT",
        "colab_type": "text"
      },
      "source": [
        "### Tanh: ###\n",
        "Hint: the derivative of $\\tanh$ is given by $\\frac{d\\tanh(z)}{d z} = 1 - \\tanh(z)^2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff6eD3dnftiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tanh(Module):            # Layer activation\n",
        "    def forward(self, Z):\n",
        "        self.A = np.tanh(Z)\n",
        "        return self.A\n",
        "\n",
        "    def backward(self, dLdA):    # Uses stored self.A\n",
        "        return None              # Your code: return dLdZ with dimensions (?, b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXAQaMmLcyTA",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FW7ocKRhcgY",
        "colab_type": "text"
      },
      "source": [
        "### ReLU: ###\n",
        "Hint:\n",
        "[`np.maximum`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.maximum.html) might be useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fm2KsLUfqdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReLU(Module):              # Layer activation\n",
        "    def forward(self, Z):\n",
        "        self.A = None            # Your code: (?, b)\n",
        "        return self.A\n",
        "\n",
        "    def backward(self, dLdA):    # uses stored self.A\n",
        "        return None              # Your code: return dLdZ (?, b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKtXuTQ0hSNO",
        "colab_type": "text"
      },
      "source": [
        "###SoftMax: ###\n",
        "For `SoftMax.class_fun()`, given the column vector of class probabilities for each point (computed by Softmax), return a vector of the classes (integers) with the highest probability for each point. The output should be a 1D numpy array. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqK-CJrnfn22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftMax(Module):           # Output activation\n",
        "    def forward(self, Z):\n",
        "        return None              # Your code: (?, b)\n",
        "\n",
        "    def backward(self, dLdZ):    # Assume that dLdZ is passed in\n",
        "        return dLdZ\n",
        "\n",
        "    def class_fun(self, Ypred):  # Return class indices\n",
        "        return None              # Your code: A 1D vector (b, ) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZc7HnMSh4fn",
        "colab_type": "text"
      },
      "source": [
        "## Loss Functions:##\n",
        "Each loss module has a forward method that takes in a batch of predictions Ypred (from the previous layer) and labels Y and returns a scalar loss value. Each column of Y will correspond to a one-hot vector encoding the correct class label for one point in our batch. \n",
        "\n",
        "The NLL module has a backward method that returns dLdZ, the gradient with respect to the preactivation to SoftMax (note: not the activation!), since we are always pairing SoftMax activation with NLL loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4uy0pHVhNd8",
        "colab_type": "text"
      },
      "source": [
        "### NLL: ###\n",
        "You should use multi-class NLL. \n",
        "\n",
        "Hint: $$\\frac{dNLL(Softmax(z))}{dz} = Y_{pred} - Y$$\n",
        "\n",
        "As an exercise, try proving that this is true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Fb8mimflgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLL(Module):       # Loss\n",
        "    def forward(self, Ypred, Y):\n",
        "        self.Ypred = Ypred\n",
        "        self.Y = Y\n",
        "        return None      # Your code\n",
        "\n",
        "    def backward(self):  # Use stored self.Ypred, self.Y\n",
        "        return None      # Your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1EffzDFkqMX",
        "colab_type": "text"
      },
      "source": [
        "## Activation and Loss Test Cases: ##\n",
        "Run Test 1 and Test 2 below and compare your outputs with the expected outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DJFzpahkvcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST 1: sgd_test for Tanh activation and SoftMax output\n",
        "np.random.seed(0)\n",
        "sgd_test(Sequential([Linear(2,3), Tanh(), Linear(3,2), SoftMax()], NLL()), test_1_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd0dXg-Qk05_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST 2: sgd_test for ReLU activation and SoftMax output\n",
        "np.random.seed(0)\n",
        "sgd_test(Sequential([Linear(2,3), ReLU(), Linear(3,2), SoftMax()], NLL()), test_2_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l5JgBU2iBCZ",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network: ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXMGcdnXgiF3",
        "colab_type": "text"
      },
      "source": [
        "Implement SGD. Randomly pick a data point Xt, Yt by using np.random.randint to choose a random index into the data. Compute the predicted output Ypred for Xt with the forward method. Compute the loss for Ypred relative to Yt. Use the backward method to compute the gradients. Use the sgd_step method to change the weights. Repeat.\n",
        "\n",
        "Hint: First call `Sequential.loss.backward()` to get dLdZ (in the case of the NLL loss function) before calling `Sequential.backward` with dLdZ as your input to propagate the loss backward through the layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejO15Vr7fhKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class Sequential:\n",
        "    def __init__(self, modules, loss):            # List of modules, loss module\n",
        "        self.modules = modules\n",
        "        self.loss = loss\n",
        "\n",
        "    def sgd(self, X, Y, iters=100, lrate=0.005):  # Train\n",
        "        D, N = X.shape\n",
        "        sum_loss = 0\n",
        "        for it in range(iters):\n",
        "            pass                                  # Your code\n",
        "\n",
        "    def forward(self, Xt):                        # Compute Ypred\n",
        "        for m in self.modules: Xt = m.forward(Xt)\n",
        "        return Xt\n",
        "\n",
        "    def backward(self, delta):                    # Update dLdW and dLdW0\n",
        "        # Note reversed list of modules\n",
        "        for m in self.modules[::-1]: delta = m.backward(delta)\n",
        "\n",
        "    def sgd_step(self, lrate):                    # Gradient descent step\n",
        "        for m in self.modules: m.sgd_step(lrate)\n",
        "\n",
        "    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n",
        "        # Utility method to print accuracy on full dataset, should\n",
        "        # improve over time when doing SGD. Also prints current loss,\n",
        "        # which should decrease over time. Call this on each iteration\n",
        "        # of SGD!\n",
        "        if it % every == 1:\n",
        "            cf = self.modules[-1].class_fun\n",
        "            acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
        "            print('Iteration =', it, '\\tAcc =', acc, '\\tLoss =', cur_loss, flush=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUojaXqphDjh",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network / SGD Test Cases: ##\n",
        "Use Test 3 and Test 4 to help you debug."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wmupM8OScodw",
        "colab": {}
      },
      "source": [
        "# TEST 3: you should achieve 100% accuracy on the hard dataset (note that we provided plotting code)\n",
        "X, Y = hard()\n",
        "nn = Sequential([Linear(2, 10), ReLU(), Linear(10, 10), ReLU(), Linear(10,2), SoftMax()], NLL())\n",
        "disp.classify(X, Y, nn, it=100000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaWfgC7Qe3ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST 4: try calling these methods that train with a simple dataset\n",
        "def nn_tanh_test():\n",
        "    np.random.seed(0)\n",
        "    nn = Sequential([Linear(2, 3), Tanh(), Linear(3, 2), SoftMax()], NLL())\n",
        "    X, Y = super_simple_separable()\n",
        "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
        "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
        "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
        "\n",
        "\n",
        "def nn_relu_test():\n",
        "    np.random.seed(0)\n",
        "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
        "    X, Y = super_simple_separable()\n",
        "    nn.sgd(X, Y, iters=2, lrate=0.005)\n",
        "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
        "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
        "\n",
        "\n",
        "def nn_pred_test():\n",
        "    np.random.seed(0)\n",
        "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
        "    X, Y = super_simple_separable()\n",
        "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
        "    Ypred = nn.forward(X)\n",
        "    return nn.modules[-1].class_fun(Ypred).tolist(), [nn.loss.forward(Ypred, Y)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dx-zM2y3R0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn_tanh_test()\n",
        "\n",
        "# Expected output:\n",
        "# '''\n",
        "# [[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n",
        "#   [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n",
        "#   [-8.47337764291184e-12, 2.6227368810847106e-09, 0.00017353185263155828]],\n",
        "#  [[0.544808855557535, -0.08366117689965663],\n",
        "#   [-0.06331837550937104, 0.24078409926389266],\n",
        "#   [0.08677202043839037, 0.8360167748667923],\n",
        "#   [-0.0037249480614718, 0.0037249480614718]]]\n",
        "# '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmYT9IWk3TQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn_relu_test()\n",
        "\n",
        "# Expected output:\n",
        "# '''\n",
        "# [[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n",
        "#   [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n",
        "#   [-0.0027754917572235106, 0.001212351486908601, -0.0005239629389906042]],\n",
        "#  [[0.501769700845158, -0.040622022187279644],\n",
        "#   [-0.09260786974986723, 0.27007359350438886],\n",
        "#   [0.08364438851530624, 0.8391444067898763],\n",
        "#   [-0.004252310922204504, 0.004252310922204505]]]\n",
        "# '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo_woDFh3a2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn_pred_test()\n",
        "\n",
        "# Expected output:\n",
        "# '''\n",
        "# ([0, 0, 0, 0], [8.56575061835767])\n",
        "# '''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}