{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Miniplaces1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ToniRV/MIT_6.862_Applied_Machine_Learning/blob/master/Miniplaces1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eAgQTTYNpA9x"
      },
      "source": [
        "# 6.869 Miniplaces Challenge - Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ErA4keUJB2TQ"
      },
      "source": [
        "The miniplaces challenge is a 2 part challenge. Each part counts for 1 pset. \n",
        "In the challenge, you will work on classifying scenes into one of several categories (such as \"desert\", or \"forest\")\n",
        "\n",
        "In this part, we'll use pretrained weights on a different dataset, but one that's also used for scene classification. We'll examine how we can visualize feature maps, to better understand how a neural net came to a decision about a particular scene.\n",
        "\n",
        "Next week, you'll implement your own neural net to do scene classification, and try to improve it as much as you can."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P6Rmom9mB1RE",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9_wzeAuZpIhG"
      },
      "source": [
        "# Requirements installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D-mfzOA1NQvl"
      },
      "source": [
        "First, let's install everything needed to run this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3GdEXtjp-QX_",
        "colab": {}
      },
      "source": [
        "!pip install Pillow==4.1.1\n",
        "!pip install -U image\n",
        "!pip install opencv-python\n",
        "\n",
        "\n",
        "from io import BytesIO\n",
        "from IPython.display import clear_output, Image, display\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "from __future__ import print_function\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gAP97sYrNa-W"
      },
      "source": [
        "We will load PyTorch, our main tool to play with neural networks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zh9nAQTrIx7V",
        "colab": {}
      },
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "import torch.hub\n",
        "\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IHEfVC-to-5V"
      },
      "source": [
        "# Loading Images and PyTorch models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pwXeYaYCNisx"
      },
      "source": [
        "Once, we have loaded all the relevant libraries, we will load the model. We will begin with an scene classification model trained on the Places Dataset with a ResNet-50 architecture.\n",
        "\n",
        "![texto alternativo](https://www.codeproject.com/KB/AI/1248963/resnet.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fHy48IdFES0L",
        "colab": {}
      },
      "source": [
        "resnet = models.resnet50(num_classes=365)\n",
        "# print(resnet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GxETSgjcC7tb",
        "colab": {}
      },
      "source": [
        "# Helper function to download things without wget\n",
        "import requests\n",
        "def download(url, fn=None):\n",
        "  if fn is None:\n",
        "    fn = url.split('/')[-1]\n",
        "  r = requests.get(url)\n",
        "  if r.status_code == 200:\n",
        "      open(fn, 'wb').write(r.content)\n",
        "      print(\"{} downloaded: {:.2f} KB\".format(fn, len(r.content)/1024.))\n",
        "  else:\n",
        "      print(\"url not found:\", url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5LYdzMo5j9cx",
        "colab": {}
      },
      "source": [
        "# Download the pretrained weights\n",
        "\n",
        "download('http://places2.csail.mit.edu/models_places365/resnet50_places365.pth.tar')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jz1hAVlpN-3y"
      },
      "source": [
        "We will load the pretrained weights into the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wabOVv7q-n9e",
        "colab": {}
      },
      "source": [
        "sd = torch.load('resnet50_places365.pth.tar') # pytorch 1.1\n",
        "sd = sd['state_dict']\n",
        "# When a model is trained on the GPU, the weights begin with \"module.\"\n",
        "# Since we aren't going to be using the GPU, we'll manually change these keys to load the state dict\n",
        "sd = {k.replace('module.', ''): v for k, v in sd.items()}\n",
        "resnet.load_state_dict(sd)\n",
        "resnet.eval()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-bF8-ICwh6z7"
      },
      "source": [
        "# Visualizing Network Filters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k6WiSqkTX_bd"
      },
      "source": [
        "First, we will define a function to display images from numpy arrays. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GqH3GvHyoq-s",
        "colab": {}
      },
      "source": [
        "def showarray(a, fmt='jpeg'):\n",
        "    a = np.uint8(np.clip(a, 0, 255))\n",
        "    f = BytesIO()\n",
        "    PIL.Image.fromarray(a).save(f, fmt)\n",
        "    display(Image(data=f.getvalue()))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oZmZT4rZE8AH",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a2G1bGXaYE6z"
      },
      "source": [
        "Now, we will focus on visualizing the filters of the ResNet network. Let's take a look to the first layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "02ltQR2-YMH9",
        "colab": {}
      },
      "source": [
        "print(resnet.conv1.weight.data.size()) # Access convolutional filters\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E3NL-WaTYN1l"
      },
      "source": [
        "Now, let's write a function to visualize the filters. You have to complete the following code, with one line normalizing the filter values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4S34CUWPkgLW",
        "colab": {}
      },
      "source": [
        "def visualize_filters(conv_w,output_size = None):\n",
        "    #TODO: Normalize conv_w values to 0-1 range  \n",
        "    w_normalized = None\n",
        "    map_t = 255*w_normalized\n",
        "    map_t = map_t.numpy()\n",
        "    map_t = map_t.astype(np.uint8)\n",
        "    if output_size is not None:\n",
        "        map_t = cv2.resize(map_t,(output_size,output_size))\n",
        "    return map_t\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E9tBWPAKYjo4"
      },
      "source": [
        "We will display the filters of the initial convolutional layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F5cWPa_4Ye89",
        "colab": {}
      },
      "source": [
        "for i in range(30):\n",
        "  print('Visualizing conv1 filter',i)\n",
        "  vis = visualize_filters(resnet.conv1.weight.data[i,0,:,:],50)\n",
        "  showarray(vis)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_iQxKhYP6NLe"
      },
      "source": [
        "## Exercise: Visualize filters for another convolutional layer in ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZqZbldo0YjAD",
        "colab": {}
      },
      "source": [
        "for i in range(30):\n",
        "  print('Visualizing conv2 filter',i)\n",
        "  vis_conv2 = visualize_filters(resnet.layer3[0].conv2.weight.data[i,0,:,:],50)\n",
        "  showarray(vis_conv2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j26-KWXXGpHP",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hykO_Di6iDI6"
      },
      "source": [
        "# Predicting classes with a pre-trained model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AkNZ-26zOQfO"
      },
      "source": [
        "To make the process easier to read, we will load the label <--> index assignament for the Places dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "44Xx19KEH2aV",
        "colab": {}
      },
      "source": [
        "# Load labels\n",
        "from urllib.request import urlopen\n",
        "\n",
        "synset_url = 'http://gandissect.csail.mit.edu/models/categories_places365.txt'\n",
        "classlabels = [r.split(' ')[0][3:] for r in urlopen(synset_url).read().decode('utf-8').split('\\n')]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZqRIs5sFZa5c"
      },
      "source": [
        "We will load one image to use through the pset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pKRBAIWRFQRq",
        "colab": {}
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "download('http://6.869.csail.mit.edu/fa19/miniplaces_part1/rio.jpg')\n",
        "img0 = PIL.Image.open('rio.jpg').convert('RGB')\n",
        "  \n",
        "img_numpy = np.array(img0)\n",
        "\n",
        "\n",
        "showarray(img_numpy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uCwHmgo1pmpF"
      },
      "source": [
        "First, let's take a look at the raw prediction of the model.\n",
        "\n",
        "You can find the ImageNet classes here: https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uCHZcX0lpvzb",
        "colab": {}
      },
      "source": [
        "  center_crop = transforms.Compose([\n",
        "         transforms.Resize((227,227)),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "  ])\n",
        "\n",
        "  im = center_crop(img0)\n",
        "  out = resnet(im.unsqueeze(0)).squeeze()\n",
        "  print(out.size())\n",
        "  categories = out.topk(5)[1]\n",
        "\n",
        "  print(categories)\n",
        "  print(classlabels[categories[0]])\n",
        "  print(classlabels[categories[1]])\n",
        "  print(classlabels[categories[2]])\n",
        "  print(classlabels[categories[3]])\n",
        "  print(classlabels[categories[4]])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "znOait62TwB5"
      },
      "source": [
        "# Visualizing Internal Activations of the Network\n",
        "\n",
        "Let's look at what parts of the image cause different units to activate (send some positive signal). All of these activations combine to inform the final inference. \n",
        "\n",
        "The convolutional layers of ResNet essentially make a semantic representation of what is contained in the image. This is followed by two fully connected layers, which use the information from that representation to categorize the image.\n",
        "\n",
        "So, let's remove the last few layers (which do classification) to get the underlying representation, and we'll visualize the activations that went into that representation from different units"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "drMqLrhbb8_4",
        "colab": {}
      },
      "source": [
        "def generate_featuremap_unit(resnet,unit_id,im_input):\n",
        "    #Extract activation from model\n",
        "    #TODO: remove the last 2 layers of resnet \n",
        "    model_cut  = None\n",
        "    # Mark the model as being used for inference\n",
        "    model_cut.eval()\n",
        "    # Crop the image\n",
        "    im = center_crop(im_input)\n",
        "    # Place the image into a batch of size 1, and use the model to get an intermediate representation\n",
        "    out = model_cut(im.unsqueeze(0))\n",
        "    # Print the shape of our representation\n",
        "    print(out.size())\n",
        "    # Extract the only result from this batch, and take just the `unit_id`th channel\n",
        "    out_final = out.squeeze()[unit_id]\n",
        "    # Return this channel\n",
        "    return out_final\n",
        "    \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v1-brxSoT6VY",
        "colab": {}
      },
      "source": [
        "def visualize_featuremap(im_input,feature_map,alpha=0.3):\n",
        "    # Normalize to [0..1], with a little leeway (0.9999) in case feature_map has 0 range\n",
        "    feature_map = feature_map/(feature_map.max()+1e-10)\n",
        "    # Convert to numpy (detach() just seperates a tensor from the gradient)\n",
        "    feat_numpy = feature_map.detach().numpy()\n",
        "    # Resize the feature map to our original image size (our strided conv layers reduce the size of the image)\n",
        "    feat_numpy = cv2.resize(feat_numpy,(im_input.shape[1],im_input.shape[0]))\n",
        "    # Invert to make the heatmap look more natural\n",
        "    map_t = 1-feat_numpy\n",
        "    # Add an extra dimension to make this a [H,W,C=1] image \n",
        "    feat_numpy = np.expand_dims(feat_numpy, axis=2)\n",
        "    \n",
        "    # Convert to image (UINT8 from 0-255)\n",
        "    map_t = 255*map_t\n",
        "    map_t = map_t.astype(np.uint8)\n",
        "    # Use a color map to change this from BW to a nice color\n",
        "    map_t = cv2.applyColorMap(map_t, cv2.COLORMAP_JET)\n",
        "    # Combine the heatmap with the original image so you can see which section of the image is activated\n",
        "    im_final = np.multiply((alpha*im_input + (1-alpha)*map_t), feat_numpy) + np.multiply(im_input, 1-feat_numpy)\n",
        "    # Return final visualization\n",
        "    return im_final\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wth2uzqQiOcg",
        "colab": {}
      },
      "source": [
        "feat = generate_featuremap_unit(resnet,300,img0)\n",
        "im_final = visualize_featuremap(img_numpy,feat)\n",
        "showarray(im_final)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VM0YGUQaJlNB",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "50sn60yjUD-u"
      },
      "source": [
        "Exercise: Find other units that detect other relevant concepts in the image. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p_kRPZ6CnILO",
        "colab": {}
      },
      "source": [
        "#TODO: Find different units "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kh6l1EaXT7JH",
        "colab": {}
      },
      "source": [
        "# (6.869 required) Find the unit index that has the maximum weights in the fully connected layer and deactivate that unit. Compare the orginal prediction and the new prediction\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "out_original = resnet(im.unsqueeze(0)).squeeze() #origianl prediction \n",
        "sorted_classes = np.argsort(-out_original.data.cpu().numpy())\n",
        "class_ids = sorted_classes[:5][0]\n",
        "print(\"top 1 class id:\", class_ids)\n",
        "# Torch.max will help - returns (maxvalue,maxindex)\n",
        "_, index = torch.max(resnet.fc.weight[class_ids,:], 0) #find the unit index that has the maximum weights in the fully connected layer \n",
        "\n",
        "\n",
        "\n",
        "#TODO: remove the last 2 layers of resnet \n",
        "model_cut = None\n",
        "\n",
        "# Get the representation for this model\n",
        "out1 = model_cut(im.unsqueeze(0))\n",
        "# Shape is now (1, # units, H, W)\n",
        "#TODO: deactive the unit that has the maximum weights (Set all values for that unit to 0)\n",
        "### DO SOMETHING HERE ###\n",
        "\n",
        "out2 = resnet.fc(resnet.avgpool(out1).squeeze().unsqueeze(0)).squeeze()\n",
        "\n",
        "def plot_top_classes(values, top_k=5, title = None):\n",
        "  sorted_classes = np.argsort(-values)\n",
        "  class_ids = sorted_classes[:top_k]\n",
        "  class_names = [classlabels[it] for it in list(class_ids)]\n",
        "  class_values = values[class_ids]\n",
        "  print(title + \" top 5 class names \", class_names)\n",
        "  print(title + \" top 5 class values \", class_values)\n",
        "  plt.bar(class_names, class_values)\n",
        "  plt.xticks(rotation=60)\n",
        "  plt.title(title)\n",
        "\n",
        "plt.figure(0)\n",
        "plot_top_classes(out.data.cpu().numpy(), title = 'Original')\n",
        "plt.figure(1)\n",
        "plot_top_classes(out2.data.cpu().numpy(), title = 'Modified')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3j8HRuNSqpmK"
      },
      "source": [
        "# Visualizing model activations with Class Activation Models (CAMs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JFgLrvzcqwzb"
      },
      "source": [
        "Once we have load the image and the model, now we will explore how to visualize the internal activations of the model. We will start by visualizing which parts of the image are responsibe for the final decision. \n",
        "\n",
        "![texto alternativo](https://camo.githubusercontent.com/fb9a2d0813e5d530f49fa074c378cf83959346f7/687474703a2f2f636e6e6c6f63616c697a6174696f6e2e637361696c2e6d69742e6564752f6672616d65776f726b2e6a7067)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w6sDff8uOmQl"
      },
      "source": [
        "We create a version of the model without the last two layers, so that we can access the last convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7i8oiFk3OKrS",
        "colab": {}
      },
      "source": [
        "#TODO: remove the last 2 layers of resnet \n",
        "model = None\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3wXXopizPDK-"
      },
      "source": [
        "We compute the activations using the Class Activation Mapping for a given output label. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "aTgSTx9QMhOB",
        "colab": {}
      },
      "source": [
        "def generate_featuremap_CAM(model,unit_id,im_input):\n",
        "    #Extract activation from model\n",
        "    \n",
        "    im = center_crop(im_input)\n",
        "    model.eval()\n",
        "    out = model(im.unsqueeze(0)) #1 x 2048 x h x w\n",
        "    w = out.size(3)\n",
        "    h = out.size(2)\n",
        "    b = out.size(0)\n",
        "    c = out.size(1)\n",
        "    print(out.size())\n",
        "    # print(b,c,h,w)\n",
        "    # fc input: N x 2048\n",
        "    \n",
        "    #TODO: convert the shape of the output (out variable) to (h*w) x c \n",
        "    # The .view() function and .transpose() functions will help\n",
        "    ### Do stuff here ###\n",
        "    \n",
        "    print(out.size())\n",
        "\n",
        "    #TODO: Run the fully connected layer from resnet to compute the weighted average with out as the input variable\n",
        "    out_final = None\n",
        "    print(out_final.size())\n",
        "    \n",
        "    out_final = out_final.view(b,h*w,-1).transpose(1,2).view(b,-1,h,w)\n",
        "    print(out_final.size())\n",
        "    out_final = out_final.squeeze()[unit_id]\n",
        "    # print(out_final.size())\n",
        "    return out_final\n",
        "    \n",
        "\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OKz611GGPP9x"
      },
      "source": [
        "We can visualize the most activated region in the image for the 5 main top classes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rksAUc1fDR-1",
        "colab": {}
      },
      "source": [
        "for i in range(categories.shape[0]):\n",
        "  print('Visualizing category',classlabels[categories[i]])\n",
        "  feat = generate_featuremap_CAM(model, categories[i].item(),img0)\n",
        "  im_result = visualize_featuremap(img_numpy,feat)\n",
        "  showarray(im_result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}