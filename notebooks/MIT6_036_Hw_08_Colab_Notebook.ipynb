{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIT6.036 Hw 08 Colab Notebook",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ToniRV/MIT_6.862_Applied_Machine_Learning/blob/master/MIT6_036_Hw_08_Colab_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xIaEwCD406A",
        "colab_type": "text"
      },
      "source": [
        "#MIT 6.036 Fall 2020: Homework 8#\n",
        "\n",
        "This colab notebook provides code and a framework for [homework 8](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week8/week8_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
        "\n",
        "## <section>**Setup**</section>\n",
        "\n",
        "First, download the code distribution for this homework that contains test cases and helper functions.\n",
        "\n",
        "Run the next code block to download and import the code for this lab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-S-l98HBVoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf code_for_hw8*\n",
        "!rm -rf data\n",
        "!rm -rf mnist_data\n",
        "!rm -rf *.zip\n",
        "!rm -rf test*/\n",
        "!rm -rf *.py\n",
        "!rm -rf *.pt\n",
        "!rm -rf __*\n",
        "!wget --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/homework/hw08/code_for_hw8.zip\n",
        "!unzip code_for_hw8.zip\n",
        "!unzip code_for_hw8/q4.zip\n",
        "!unzip -q test1.zip\n",
        "!unzip -q test2.zip\n",
        "!unzip -q test3.zip\n",
        "!mv code_for_hw8/* .\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "import math as m \n",
        "\n",
        "import torch\n",
        "from torch.nn import (Linear, ReLU, Conv1d, Flatten, Conv2d, Sequential, \n",
        "                      MaxPool1d, MaxPool2d, Dropout, CrossEntropyLoss, MSELoss)\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision\n",
        "\n",
        "from skimage.io import imread, imshow\n",
        "from skimage.transform import resize\n",
        "\n",
        "import os\n",
        "\n",
        "from code_for_hw8_pytorch import get_image_data_1d\n",
        "\n",
        "from utils_hw8 import (model_fit, model_evaluate, run_pytorch, call_model, \n",
        "                       plot_decision, plot_heat, plot_separator, make_iter, \n",
        "                       set_weights, set_bias)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-sSs7N4mMiX",
        "colab_type": "text"
      },
      "source": [
        "# 1) Implementing Mini-batch Gradient Descent and Batch Normalization\n",
        "\n",
        "** Note: You can click the arrow on the left of this text block to collapse/expand this optional section and all its code blocks **\n",
        "\n",
        "Last week we implemented a framework for building neural networks from scratch. We trained our models using *stochastic* gradient descent. In this problem, we explore how we can implement batch normalization as a module `BatchNorm` in our framework. It is the same module which you analyzed in problem 1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgxmIfXVmVwd",
        "colab_type": "text"
      },
      "source": [
        "Key to the concept of batch normalization is the doing gradient descent on batches of data. So we instead of using last week's stochastic gradient descent, we will first implement the *mini-batch* gradient descent method `mini_gd`, which is a hybrid between *stochastic* gradient descent and *batch* gradient descent. The lecture notes on <a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week7/neural_networks_2/1?activate_block_id=block-v1%3AMITx%2B6.036%2B2019_Spring%2Btype%40vertical%2Bblock%40neural_networks_2_optimizing_neural_network_parameters_vert\"> optimizing neural network parameters</a> are helpful for this part.\n",
        "\n",
        "In *mini-batch* gradient descent, for a mini-batch of size $K$, we select $K$ distinct data points uniformly at random from the data set and update the network weights based only on their contributions to the gradient:\n",
        "$$W := W - \\eta\\sum_{i=1}^K \\nabla_W \\mathcal{L}(h(x^{(i)}; W), y^{(i)})\\;\\;.$$\n",
        "\n",
        "Our *mini-batch* method `mini_gd` will be implemented within the `Sequential` python class (see homework 7 problem 2) and will take the following as inputs:\n",
        "\n",
        "* `X`: a standard data array (d by n)\n",
        "* `y`: a standard labels row vector (1 by n)\n",
        "* `iters`: the number of updates to perform on weights $W$\n",
        "* `lrate`: the learning rate used\n",
        "* `K`: the mini-batch size to be used\n",
        "\n",
        "One call of `mini_gd` should call `Sequential.backward` for back-propagation and `Sequential.step` for updating the weights, for a total of `iters` times, using `lrate` as the learning rate. As in our implementation of `sgd` from homework 7, we compute the predicted output for a mini-batch of data with the `Sequential.forward` method. We compute the loss between our predictions and the true labels using the assigned `Sequential.loss` method. (Note that in homework 7, `Sequential.step` was called `Sequential.sgd_step`. While the functionality of the step function is the same, it has been renamed for convenience. The same is true for the `module.step` function of each module we implemented, where applicable.)\n",
        "\n",
        "For picking $K$ unique data points at random from our large data-set for each mini-batch, we will implement the following strategy: we will first shuffle our data points `X` (and associated labels `y`). Then, we get $\\frac{n}{k}$ (rounded down to the nearest integer) different mini-batches by grouping each $K$ consecutive points from this shuffled array. If we end up iterating over all the points but need more mini-batches, we will repeat the shuffling and the batching process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr1kWI08mdo4",
        "colab_type": "text"
      },
      "source": [
        "<b>1A)</b> You need to fill in the missing code below. We have implemented the shuffling of indices and have provided you with the outer and inner loops."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_lvmO9Z22bH",
        "colab_type": "text"
      },
      "source": [
        "### PLEASE COPY IN YOUR CODE FROM HOMEWORK 7 TO COMPLEMENT THE CLASSES GIVEN HERE\n",
        "\n",
        "Recall that your implementation from homework 7 included the following classes:\n",
        "    \n",
        "  * Module\n",
        "  * Linear \n",
        "  * Tanh \n",
        "  * ReLU \n",
        "  * SoftMax\n",
        "  * NLL\n",
        "  * Sequential"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y-Tigvq4gOEs",
        "colab": {}
      },
      "source": [
        "import math as m\n",
        "class Module:\n",
        "    def sgd_step(self, lrate): pass  # For modules w/o weights\n",
        "\n",
        "\n",
        "class Linear(Module):\n",
        "    def __init__(self, m, n):\n",
        "        self.m, self.n = (m, n)  # (in size, out size)\n",
        "        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
        "        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n",
        "\n",
        "    def forward(self, A):\n",
        "        self.A = A   # (m x b)  Hint: make sure you understand what b stands for\n",
        "        return None  # Your code (n x b)\n",
        "\n",
        "    def backward(self, dLdZ):  # dLdZ is (n x b), uses stored self.A\n",
        "        self.dLdW = None       # Your code\n",
        "        self.dLdW0 = None      # Your code\n",
        "        return None            # Your code: return dLdA (m x b)\n",
        "\n",
        "    def sgd_step(self, lrate):  # Gradient descent step\n",
        "        self.W = None           # Your code\n",
        "        self.W0 = None          # Your code\n",
        "\n",
        "\n",
        "class Tanh(Module):            # Layer activation\n",
        "    def forward(self, Z):\n",
        "        self.A = np.tanh(Z)\n",
        "        return self.A\n",
        "\n",
        "    def backward(self, dLdA):    # Uses stored self.A\n",
        "        return None              # Your code: return dLdZ\n",
        "\n",
        "\n",
        "class ReLU(Module):              # Layer activation\n",
        "    def forward(self, Z):\n",
        "        self.A = None            # Your code\n",
        "        return self.A\n",
        "\n",
        "    def backward(self, dLdA):    # uses stored self.A\n",
        "        return None              # Your code: return dLdZ\n",
        "\n",
        "\n",
        "class SoftMax(Module):           # Output activation\n",
        "    def forward(self, Z):\n",
        "        return None              # Your code\n",
        "\n",
        "    def backward(self, dLdZ):    # Assume that dLdZ is passed in\n",
        "        return dLdZ\n",
        "\n",
        "    def class_fun(self, Ypred):  # Return class indices\n",
        "        return None              # Your code\n",
        "\n",
        "\n",
        "class NLL(Module):       # Loss\n",
        "    def forward(self, Ypred, Y):\n",
        "        self.Ypred = Ypred\n",
        "        self.Y = Y\n",
        "        return None      # Your code\n",
        "\n",
        "    def backward(self):  # Use stored self.Ypred, self.Y\n",
        "        return None      # Your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHso5_RFrgnF",
        "colab_type": "text"
      },
      "source": [
        "Implement `mini_gd` in `Sequential` below.\n",
        "\n",
        "**Hint:** The documentation for <a href=\"https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.shuffle.html\"> `numpy.random.shuffle`</a> might be helpful for this part. If you have a list of elements `l` and a set of indices `indices`, you can call `numpy.random.shuffle(indices)` and `l = l[indices]` to shuffle the elements of `l`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brq8yO4trfCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sequential:\n",
        "    def __init__(self, modules, loss):            \n",
        "        self.modules = modules\n",
        "        self.loss = loss\n",
        "\n",
        "    def mini_gd(self, X, Y, iters, lrate, notif_each=None, K=10):\n",
        "        D, N = X.shape\n",
        "\n",
        "        np.random.seed(0)\n",
        "        num_updates = 0\n",
        "        indices = np.arange(N)\n",
        "        while num_updates < iters:\n",
        "\n",
        "            np.random.shuffle(indices)\n",
        "            X = None  # Your code\n",
        "            Y = None  # Your code\n",
        "\n",
        "            for j in range(m.floor(N/K)):\n",
        "                if num_updates >= iters: break\n",
        "\n",
        "                # Implement the main part of mini_gd here\n",
        "                # Your code\n",
        "                Xt = None  # Your code\n",
        "                Yt = None  # Your code\n",
        "\n",
        "                # The rest of this function should be similar to your\n",
        "                # implementation of Sequential.sgd in HW 7\n",
        "                # Your code\n",
        "                \n",
        "                num_updates += 1\n",
        "\n",
        "    def forward(self, Xt):                        \n",
        "        for m in self.modules: Xt = m.forward(Xt)\n",
        "        return Xt\n",
        "\n",
        "    def backward(self, delta):                   \n",
        "        for m in self.modules[::-1]: delta = m.backward(delta)\n",
        "\n",
        "    def sgd_step(self, lrate):    \n",
        "        for m in self.modules: m.sgd_step(lrate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JZeeKXkm6YI",
        "colab_type": "text"
      },
      "source": [
        "<b>1B)</b> We are now ready to implement batch normalization into our neural network framework! Our module `BatchNorm` will sit between consecutive layers of neurons, such as the $l^{th}$ and $(l+1)^{th}$ layers, acting as a \"corrector\" which allows $W^l$ to change freely, producing outputs $z^l$, but then the module corrects the covariate shift induced in the signals before they reach the $(l+1)^{th}$ layer, converting $z^l$ to $\\widehat{Z}^l$. \n",
        "\n",
        "The following is a summmary what is described in the <a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week7/neural_networks_2/2\">lecture notes</a>, and it should guide your implementation of the module. \n",
        "\n",
        "Any normalization between the $l^{th}$ and $(l+1)^{th}$ layers is done *separately* for each of the $n^l$ input connections leading to the $(l+1)^{th}$ layer. We handle a mini-batch of data of size $K$, and $Z^l$ is $n^l \\times K$, and the output $\\widehat{Z}^l$is of the same shape. \n",
        "\n",
        "We first compute $n^l$ *batchwise* means and\n",
        "standard deviations.  Let $\\mu^l$ be the $n^l \\times 1$ vector (`self.mus`) where\n",
        "$$\\mu^l_i = \\frac{1}{K} \\sum_{j = 1}^K Z^l_{ij}\\;\\;,$$\n",
        "and let $\\sigma^l$ be the $n^l \\times 1$ vector (`self.vars`) where \n",
        "$$\\sigma^l_i = \\sqrt{\\frac{1}{K} \\sum_{j = 1}^K (Z^l_{ij} - \\mu_i)^2}\\;\\;.$$\n",
        "\n",
        "The normalized data `self.norm` is the matrix $\\overline{Z}$, where\n",
        "$$\\overline{Z}^l_{ij} = \\frac{Z^l_{ij} - \\mu^l_i}{\\sigma^l_i + \\epsilon}\\;\\;,$$\n",
        "and where $\\epsilon$ is a very small constant to guard against division by\n",
        "zero. \n",
        "\n",
        "We define weights $G^l$ (`self.G`) and $B^l$ (`self.B`), each being an $n^l \\times 1$ vector, which we use to to shift and scale the outputs:\n",
        "$$\\widehat{Z}^l_{ij} = G^l_i \\overline{Z}^l_{ij} + B^l_i\\;\\;.$$\n",
        "\n",
        "The outputs are finally ready to be passed to the $(l+1)^{th}$ layer.\n",
        "\n",
        "A slight warning (that we will not worry about here) about `BatchNorm` is that during the *test* phase, if the test mini-batch size is too small (imagine we are deploying a neural network that deals with live video frames), then the lack of samples would cause the freshly-calculated $\\mu^l$ and $\\sigma^l$ to be far off from their true values that the module's parameters $G^l$ and $B^l$ were trained to be compatible with. To fix that, people usually compute a running average of $\\mu^l$ and $\\sigma^l$ during training, to be used at test time. We will assume our test mini-batches are large enough.\n",
        "\n",
        "In this problem we only implement the `BatchNorm.forward` and `BatchNorm.step` methods. We provide you with the implementation for `BatchNorm.backward` and the lecture notes contain the details of the derivations. You will need to fill in the missing code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlXP26plm8R7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BatchNorm(Module):    \n",
        "    def __init__(self, m):\n",
        "        np.random.seed(0)\n",
        "        self.eps = 1e-20\n",
        "        self.m = m  # number of input channels\n",
        "        \n",
        "        # Init learned shifts and scaling factors\n",
        "        self.B = np.zeros([self.m, 1]) # m x 1\n",
        "        self.G = np.random.normal(0, 1.0 * self.m ** (-.5), [self.m, 1]) # m x 1\n",
        "        \n",
        "    # Works on m x b matrices of m input channels and b different inputs\n",
        "    def forward(self, A):# A is m x K: m input channels and mini-batch size K\n",
        "        # Store last inputs and K for next backward() call\n",
        "        self.A = A\n",
        "        self.K = A.shape[1]\n",
        "        \n",
        "        self.mus = None  # Your Code\n",
        "        self.vars = None  # Your Code\n",
        "\n",
        "        # Normalize inputs using their mean and standard deviation\n",
        "        self.norm = None  # Your Code\n",
        "            \n",
        "        # Return scaled and shifted versions of self.norm\n",
        "        return None  # Your Code\n",
        "\n",
        "    def backward(self, dLdZ):\n",
        "        # Re-usable constants\n",
        "        std_inv = 1/np.sqrt(self.vars+self.eps)\n",
        "        A_min_mu = self.A-self.mus\n",
        "        \n",
        "        dLdnorm = dLdZ * self.G\n",
        "        dLdVar = np.sum(dLdnorm * A_min_mu * -0.5 * std_inv**3, axis=1, keepdims=True)\n",
        "        dLdMu = np.sum(dLdnorm*(-std_inv), axis=1, keepdims=True) + dLdVar * (-2/self.K) * np.sum(A_min_mu, axis=1, keepdims=True)\n",
        "        dLdX = (dLdnorm * std_inv) + (dLdVar * (2/self.K) * A_min_mu) + (dLdMu/self.K)\n",
        "        \n",
        "        self.dLdB = np.sum(dLdZ, axis=1, keepdims=True)\n",
        "        self.dLdG = np.sum(dLdZ * self.norm, axis=1, keepdims=True)\n",
        "        return dLdX\n",
        "\n",
        "    def step(self, lrate):\n",
        "        self.B = None  # Your Code\n",
        "        self.G = None  # Your Code\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh4u39OCjLza",
        "colab_type": "text"
      },
      "source": [
        "# 2) Weight sharing (OPTIONAL)\n",
        "\n",
        "** Note: You can click the arrow on the left of this text block to collapse/expand this optional section and all its code blocks **\n",
        "\n",
        "In the lab we designed a CNN that can count the number of objects in 1 dimensional images, where each black pixel is represented by a value of 0 and each white pixel is represented by a value of 1. Recall that an object is a consecutive sequence of black pixels ($0$'s). For example, the sequence $0100110$ contains three objects. \n",
        "\n",
        "In this problem we want to see how hard/easy it is to train such a network from data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp8stejLA57F",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Our network architecture will be as follows:\n",
        "\n",
        "*    The first layer is convolutional and you will implement it using the PyTorch `torch.nn.Conv1d` function, with a kernel of size 2 and stride of 1, followed by a ReLu activation (`torch.nn.ReLU`).\n",
        "\n",
        "*    The second layer is a fully connected `torch.nn.Linear` layer which has a scalar output.\n",
        "\n",
        "Here is sample usage of the `Conv1d` and `Linear` layers. \n",
        "\n",
        "`layer1=torch.nn.Conv1d(in_channels=?, out_channels=?, kernel_size=?,stride=?,padding=?,bias=True)`\n",
        "\n",
        "Here, `in_channels` is the number of channels in your data (so for example, RGB images have 3 channels). You can think of the `out_channels` variable as the number of filters you are using.\n",
        "\n",
        "`layer3 = torch.nn.Linear(in_units=?, out_units=?)`\n",
        "\n",
        "You need to fill in the parameters marked with `?` based on the problem specifications. Note also that in PyTorch, depending on your implementation, you may be forced to use *three* (four if we count ReLU) layers to implement such a network, where one intermediary `Flatten` layer is used to flatten the output of the convolutional layer, before being passed to the dense layer.\n",
        "\n",
        "Refer to the <a href=\"https://pytorch.org/docs/stable/nn.html#conv1d\">Conv1D</a>, <a href=\"https://pytorch.org/docs/stable/nn.html#linear\">Linear</a> and <a href=\"https://pytorch.org/docs/stable/nn.html#flatten\">Flatten</a> descriptions in the PyTorch documentation to see the available parameter options.\n",
        "\n",
        "In this exercise, we fix the structure and want to learn the best combination of weights from data. In the homework code, we have provided functions `train_neural_counter` and `get_image_data_1d`. You can use them to generate data and train the above neural network in PyTorch to answer the following questions. We assume that the images in our data set are randomly generated. The probability of a pixel being white is $0.1$. We work with mean squared error as the loss function for this problem. We have provided template code which you can fill in, to perform the training.\n",
        "\n",
        "We have also provided helper functions such as `set_weight`, `set_bias`, which might help you set weights and biases of a particular layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKa8iMv_j3ek",
        "colab_type": "text"
      },
      "source": [
        "<b>2B)</b> What is (approximately) the expected loss of the network on $1024\\times 1$ images if the convolutional layer is an averaging filter and second layer is the sum function (without a bias term)? (Note that you can answer the question theoretically or through coding, depending on your preference.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKPcB588ok8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code template if you would like to check 2B) through code\n",
        "\n",
        "tsize = 1000\n",
        "imsize = 1024\n",
        "prob_white = 0.1\n",
        "\n",
        "(X_train,Y_train,X_val,Y_val,X_test,Y_test) = get_image_data_1d(tsize,imsize,prob_white)\n",
        "test_loader = make_iter(X_test, Y_test)\n",
        "\n",
        "num_filters = None  # Your code\n",
        "kernel_size = None  # Your code\n",
        "strides = None  # Your code\n",
        "padding = None # Your code\n",
        "\n",
        "layer_1 = Conv1d(in_channels=1, out_channels=num_filters, kernel_size=kernel_size, stride=strides, padding=padding, bias=False)\n",
        "\n",
        "num_units = imsize+1  # Your code\n",
        "layer_3 = Linear(num_units, 1, bias=False)\n",
        "layers = [layer_1, ReLU(), Flatten(), layer_3]\n",
        "model = Sequential(*layers)\n",
        "\n",
        "set_weights(model[0], np.array([1/2,1/2]))\n",
        "set_weights(model[-1], np.ones(num_units))\n",
        "\n",
        "model_evaluate(model, test_loader, MSELoss())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js3OYsbwj7Ms",
        "colab_type": "text"
      },
      "source": [
        "<b>2C)</b> Now suppose we add a bias term of $-10$ to the last layer. What is (approximately) the expected quadratic loss? (Note that you can answer the question theoretically or through coding, depending on your preference.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKynxhF1klga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Edit code from 2B) with the bias\n",
        "bias = None # Your code here\n",
        "set_bias(model[-1], bias)\n",
        "model_evaluate(model, test_loader, MSELoss())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GCLr8qmj-Hk",
        "colab_type": "text"
      },
      "source": [
        "<b>2D)</b> Averaging type filters are abundant and form a nearly flat valley of local minima for this problem. It is difficult for the network to find alternative solutions on its own. We need to force our way out of these bad minima and towards a better solution, i.e., an edge detector. To force the first layer to behave as an edge detector, we need to choose a proper **kernel regularizer**. Consider the following functions\n",
        "\n",
        "$f_1=\\sum_i |w_i|$, $f_2=\\sum_i |w_i^2|$, $f_3=|\\sum_{i} w_i|$. Which one of the choices is likely to guide the network to find an edge detector at the convolution layer?\n",
        "\n",
        "\n",
        "<a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2020_Spring/courseware/Week8/week8_homework/\">Refer to HW8 on MITx.</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aubU6Q6kwOI",
        "colab_type": "text"
      },
      "source": [
        "Implement your choice of regularizers from above in the code (complete the function `filter_reg`). Do not allow any bias in the layers for the rest of the problem. The code generates some random test and training data sets and trains the model on these data. Run a few learning trials (5 or more) for each data set and answer the following questions based on the performance of your model.\n",
        "\n",
        "**IMPORTANT**: When implementing `filter_reg`, you should use the torch backend operations, imported as \"torch\" in the code. So for example, `torch.sum` and `torch.abs`, rather than `np.sum` and `np.abs`. This is because the `weights` argument is NOT a numpy object, but rather an internal torch object!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOLZf_JsuTLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implement filter_reg\n",
        "def filter_reg(weights):\n",
        "    # We scale the output of the filter by lam\n",
        "    lam=1000\n",
        "    filter_result = None  # Your code\n",
        "    return lam * filter_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bXvcRuk_3q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_reg(model):\n",
        "    # Don't edit this function!\n",
        "    filter_weights = model[0].weight\n",
        "    return filter_reg(filter_weights)\n",
        "\n",
        "def train_neural_counter(layers, data, regularize=False, display=False):\n",
        "    (X_train, Y_train, X_val, Y_val, X_test, Y_test) = data\n",
        "    epochs = 10\n",
        "    batch = 1\n",
        "\n",
        "    train_iter, val_iter, test_iter = (make_iter(X_train, Y_train),\n",
        "                                       make_iter(X_val,Y_val),\n",
        "                                       make_iter(X_test,Y_test))\n",
        "    model = Sequential(*layers)\n",
        "    optimizer = Adam(model.parameters())\n",
        "    criterion = MSELoss()\n",
        "\n",
        "    model_fit(model, train_iter, epochs, optimizer, criterion, val_iter, \n",
        "              history=None,verbose=True, model_reg=model_reg if regularize else None)\n",
        "    err = model_evaluate(model, test_iter, criterion)\n",
        "    ws = model[-1].weight\n",
        "    if display:\n",
        "        plt.plot(ws)\n",
        "        plt.show()\n",
        "    return model,err"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYRwd0eJkAdh",
        "colab_type": "text"
      },
      "source": [
        "<b>2E)</b> For $1024\\times 1$ images and training set of size $1000$, is the network **without any regularization** likely to find models that have a mean square error lower than 8 on the test data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KiCbZmksXO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imsize = 1024\n",
        "prob_white = 0.1\n",
        "\n",
        "data=get_image_data_1d(1000, imsize, prob_white)\n",
        "trials=5\n",
        "\n",
        "\n",
        "for trial in range(trials):\n",
        "    num_filters = None  # Your code\n",
        "    kernel_size = None  # Your code\n",
        "    strides = None  # Your code\n",
        "    padding = None # Your code\n",
        "\n",
        "    layer_1 = Conv1d(in_channels=1, out_channels=num_filters, kernel_size=kernel_size, stride=strides, padding=padding, bias=False)\n",
        "\n",
        "    num_units = None  # Your code\n",
        "    layer_3 = Linear(num_units, 1, bias=False)\n",
        "    layers = [layer_1, ReLU(), Flatten(), layer_3]\n",
        "    model, err=train_neural_counter(layers, data)\n",
        "    print(model[0].weight)\n",
        "    print(torch.mean(model[-1].weight))\n",
        "    print(err)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1vcUEL-vW9D",
        "colab_type": "text"
      },
      "source": [
        "#### For parts F) to J), simply edit your code from E) with the necessary changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_25ygQJkD5F",
        "colab_type": "text"
      },
      "source": [
        "<b>2F)</b> Repeat the same experiment, but now with the regularizer you implemented. Try different regularization parameters. Which choice of regularization parameter gives the best prediction results?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNAChIqylIlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Edit code from 2E), using your regularization turned on in the train_neural_counter function.\n",
        "# Try setting the lambda parameter to different values in filter_reg\n",
        "# Set regularize=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs44ze96kHZZ",
        "colab_type": "text"
      },
      "source": [
        "<b>2G)</b> With the above choice of regularization parameter, what is the mean square error of the best network that you find on the test data? Try a few trials (5 or more) for each data test and report the value of the best network. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAN0k9wylOmz",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### We expect the training to be easier when there are fewer parameters to learn. Consider images of size $128\\times 1$ for the rest of the problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnktFwXRkKNF",
        "colab_type": "text"
      },
      "source": [
        "<b>2H)</b> Instead of resorting to regularization again, we may instead find a way to reduce the number of parameters. What additional layer can you add to the output of the convolution layer to reduce the number of parameters to be learned without losing any relevant information?\n",
        "\n",
        "<a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2020_Spring/courseware/Week8/week8_homework/\">Refer to HW8 on MITx.</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXgOqKtRkNRP",
        "colab_type": "text"
      },
      "source": [
        "<b>2I)</b> Add the layer you suggested above to your network and run some tests with data sets of size 1000 on $128\\times 1$ images.  How many parameters are left to learn with the new structure?\n",
        "\n",
        "You can find the appropriate documentations for the new types of modules mentioned in the previous problem here:\n",
        "\n",
        "<a href=\"https://pytorch.org/docs/stable/nn.html#dropout\">Dropout</a>\n",
        "\n",
        "<a href=\"https://pytorch.org/docs/stable/nn.html#maxpool1d\">MaxPool1d</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQGlJLxI__4A",
        "colab_type": "text"
      },
      "source": [
        "# 3) MNIST (Digit Classification)\n",
        "\n",
        "In this section, we'll be looking at the MNIST data set seen already in problem 2. This time, we look at the *complete* MNIST problem where our networks will take an image of *any* digit from $0-9$ as input (recall that problem 2 only looked at digits $0$ and $1$) and try to predict that digit. Note that in general, an image is described as a two-dimensional array of pixels. Here, the image is a <a href=\"https://en.wikipedia.org/wiki/Grayscale\">grayscale</a> image, so each pixel is represented by only one integer value, in the range $0$ to $255$ (compared to RGB images where each pixel is represented by three integer values, encoding intensity levels in red, green, and blue color channels).\n",
        "\n",
        "Also, we will now use out-of-the-box neural network implementations using PyTorch. State-of-the-art systems have error rates of less than 0.5% percent on this data set (see <a href=\"http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354\">this list</a>).  We'll be happy with an error rate less than 2% since we don't have all year...\n",
        "\n",
        "You can access the MNIST data for this problem using:\n",
        "<br><code>train, validation = get_MNIST_data()</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrIFeBFNWu38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shifted(X, shift):\n",
        "    n = X.shape[0]\n",
        "    m = X.shape[1]\n",
        "    size = m + shift\n",
        "    X_sh = np.zeros((n, size, size))\n",
        "    plt.ion()\n",
        "    for i in range(n):\n",
        "        sh1 = np.random.randint(shift)\n",
        "        sh2 = np.random.randint(shift)\n",
        "        X_sh[i, sh1:sh1+m, sh2:sh2+m] = X[i, :, :]\n",
        "        # If you want to see the shifts, uncomment\n",
        "        #plt.figure(1); plt.imshow(X[i])\n",
        "        #plt.figure(2); plt.imshow(X_sh[i])\n",
        "        #plt.show()\n",
        "        #input('Go?')\n",
        "    return X_sh\n",
        "  \n",
        "def get_MNIST_data(shift=0):\n",
        "    train = MNIST(root='./mnist_data', train=True, download=True, transform=None)\n",
        "    val = MNIST(root='./mnist_data', train=False, download=True, transform=None)\n",
        "    (X_train, y1), (X_val, y2) = (train.data.numpy(), train.targets.numpy()), \\\n",
        "                                  (val.data.numpy(), val.targets.numpy())\n",
        "    if shift:\n",
        "        X_train = shifted(X_train, shift)\n",
        "        X_val = shifted(X_val, shift)\n",
        "    return (X_train, y1), (X_val, y2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZvlHZS3WjRA",
        "colab_type": "text"
      },
      "source": [
        "You can run the fully connected MNIST model, using:\n",
        "<br><code>run_pytorch_fc_mnist(train, validation, layers, epochs)</code>\n",
        "\n",
        "And, you can run the CNN MNIST test, using:\n",
        "<br><code>run_pytorch_cnn_mnist(train, validation, layers, epochs)</code>\n",
        "\n",
        "You will need to design your own `layers` to feed to `run_pytorch_fc_mnist` and `run_pytorch_cnn_mnist`, which will be different than the ones specified by `archs()`. For instance, `layers=[Linear(in_features=64, out_features=4)]` defines a single layer with 64 inputs and 4 output units.\n",
        "Note that the training procedure, uses <a href=\"https://pytorch.org/docs/stable/nn.html#crossentropyloss\">PyTorch's CrossEntropyLoss</a>, which handles the softmax activations for you, so adding a softmax layer to the end of your network is not necessary and will produce undesired results.\n",
        "Also, we advise you to use the option `verbose=True` when unsure about the progress made during training of your models.\n",
        "#### **IMPORTANT:** For this and subsequent questions, use the PyTorch implementation of modules. For example, for a linear layer, use <code>nn.Linear(...)</code>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx1jt6P9AUk1",
        "colab_type": "text"
      },
      "source": [
        "<b> 3A)</b> Look at the code and indicate what the difference is between <code>run_pytorch_fc_mnist</code> and <code>run_pytorch_cnn_mnist</code>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5moSfb7CcXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_deterministic():\n",
        "    torch.manual_seed(10)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(10)\n",
        "\n",
        "def weight_reset(l):\n",
        "    if isinstance(l, Conv2d) or isinstance(l, Linear):\n",
        "        l.reset_parameters()\n",
        "\n",
        "def run_pytorch_fc_mnist(train, test, layers, epochs, verbose=True, trials=1, deterministic=True):\n",
        "    '''\n",
        "    train, test = input data\n",
        "    layers = list of PyTorch layers, e.g. [Linear(in_features=784, out_features=10)]\n",
        "    epochs = number of epochs to run the model for each training trial\n",
        "    trials = number of evaluation trials, resetting weights before each trial\n",
        "    '''\n",
        "    if deterministic:\n",
        "        make_deterministic()\n",
        "    (X_train, y1), (X_val, y2) = train, test\n",
        "    # Flatten the images\n",
        "    m = X_train.shape[1]\n",
        "    X_train = X_train.reshape((X_train.shape[0], m * m))\n",
        "    X_val = X_val.reshape((X_val.shape[0], m * m))\n",
        "\n",
        "    val_acc, test_acc = 0, 0\n",
        "    for trial in range(trials):\n",
        "        # Reset the weights\n",
        "        for l in layers:\n",
        "            weight_reset(l)\n",
        "        # Make Dataset Iterables\n",
        "        train_iter, val_iter = make_iter(X_train, y1, batch_size=32), make_iter(X_val, y2, batch_size=32)\n",
        "        # Run the model\n",
        "        model, vacc, tacc = \\\n",
        "            run_pytorch(train_iter, val_iter, None, layers, epochs, verbose=verbose)\n",
        "        val_acc += vacc if vacc else 0\n",
        "        test_acc += tacc if tacc else 0\n",
        "    if val_acc:\n",
        "        print(\"\\nAvg. validation accuracy:\" + str(val_acc / trials))\n",
        "    if test_acc:\n",
        "        print(\"\\nAvg. test accuracy:\" + str(test_acc / trials))\n",
        "\n",
        "\n",
        "def run_pytorch_cnn_mnist(train, test, layers, epochs, verbose=True, trials=1, deterministic=True):\n",
        "    if deterministic:\n",
        "        make_deterministic()\n",
        "    # Load the dataset\n",
        "    (X_train, y1), (X_val, y2) = train, test\n",
        "    # Add a final dimension indicating the number of channels (only 1 here)\n",
        "    m = X_train.shape[1]\n",
        "    X_train = X_train.reshape((X_train.shape[0], 1, m, m))\n",
        "    X_val = X_val.reshape((X_val.shape[0], 1, m, m))\n",
        "\n",
        "    val_acc, test_acc = 0, 0\n",
        "    for trial in range(trials):\n",
        "        # Reset the weights\n",
        "        for l in layers:\n",
        "            weight_reset(l)\n",
        "        # Make Dataset Iterables\n",
        "        train_iter, val_iter = make_iter(X_train, y1, batch_size=32), make_iter(X_val, y2, batch_size=32)\n",
        "        # Run the model\n",
        "        model, vacc, tacc = \\\n",
        "            run_pytorch(train_iter, val_iter, None, layers, epochs, verbose=verbose)\n",
        "        val_acc += vacc if vacc else 0\n",
        "        test_acc += tacc if tacc else 0\n",
        "    if val_acc:\n",
        "        print(\"\\nAvg. validation accuracy:\" + str(val_acc / trials))\n",
        "    if test_acc:\n",
        "        print(\"\\nAvg. test accuracy:\" + str(test_acc / trials))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sGfqAbICbmE",
        "colab_type": "text"
      },
      "source": [
        "<b> 3B)</b> Using one epoch of training, what is the accuracy of a network **with no hidden units** (using the <code>run_pytorch_fc_mnist</code> method) on this data?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1VAxZ17DtPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, validation = get_MNIST_data()\n",
        "run_pytorch_fc_mnist(train, validation, [nn.Linear(in_features=28*28, out_features=10)], 1, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsQ31e1lDE6u",
        "colab_type": "text"
      },
      "source": [
        "<b> 3C)</b> Now, linearly scale the data so that the pixel values are between 0 and 1 and repeat your test with the original layer. What is the accuracy now?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b-RcZu1EPaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layers = None # Your code\n",
        "train, validation = get_MNIST_data()\n",
        "\n",
        "# Scale the images\n",
        "train = None # Your code\n",
        "validation = None # Your code\n",
        "\n",
        "run_pytorch_fc_mnist(train, validation, layers, 1, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A__3x6eyEIFn",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrYGfcOLEr0f",
        "colab_type": "text"
      },
      "source": [
        "### Important: <b>Always scale the data like in 3C) for subsequent problems.</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHoyqJdqDFH5",
        "colab_type": "text"
      },
      "source": [
        "<b> 3E)</b> Using this same architecture, what is the accuracy after the 1st, 5th, 10th, and 15th epochs? Note that this colab notebook 0-indexes epoch output. We're looking for the first, fifth, tenth, and fifteenth number outputted by <code>run_pytorch_fc_mnist(train, validation, layers, 15, verbose=True)</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8PlbWS_EwTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layers = None # Your code\n",
        "run_pytorch_fc_mnist(train, validation, layers, 15, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCPTuz-tDFTg",
        "colab_type": "text"
      },
      "source": [
        "<b> 3H)</b> Using one epoch of training, try a single hidden layer, followed by a ReLU activation layer before the final output layer, and gradually increase the units; specifically, try (128, 256, 512, 1024) units and observe the results.  What are the accuracies?\n",
        "To define a ReLU layer in pytorch simply use <code>ReLU()</code>.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmpbP_VHFoh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for num in [128,256,512,1024]:\n",
        "    layers = None  # Your code\n",
        "    run_pytorch_fc_mnist(train, validation, layers, 1, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEM5mZi5DFYS",
        "colab_type": "text"
      },
      "source": [
        "<b> 3I)</b> Now, try a network with two hidden layers:\n",
        "<ul>\n",
        "  <li>A fully connected layer with 512 hidden units\n",
        "  <li> A ReLU activation layer\n",
        "  <li> A fully connected layer with 256 hidden units\n",
        "  <li> A ReLU activation layer\n",
        "  <li> A fully-connected layer with 10 output units\n",
        "    \n",
        "What is the accuracy?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cwp1VR7F06q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layers = None  # Your code\n",
        "run_pytorch_fc_mnist(train, validation, layers, 1, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmnNUT2nDFdi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<b> 3J)</b> Build a convolutional network with the following structure:\n",
        "\n",
        "<ul>\n",
        "<li> A convolutional layer with 32 filters of size 3 × 3\n",
        "<li> A ReLU activation layer\n",
        "<li> A max pooling layer with size 2 × 2\n",
        "<li> A convolutional layer with 64 filters of size 3 × 3\n",
        "<li> A ReLU activation layer\n",
        "<li> A max pooling layer with size 2 × 2\n",
        "<li> A flatten layer (<b>will flatten to size 1600</b>; try to figure out why!)\n",
        "<li> A fully connected layer with 128 units \n",
        "<li> A ReLU activation layer\n",
        "<li> A dropout layer with drop probability 0.5\n",
        "<li> A fully-connected layer with 10 output units\n",
        "</ul>\n",
        "To define Convolutional and max pooling layers in PyTorch use the following syntax:\n",
        "<code> c = Conv2d(in_channels=i, out_channels=o, kernel_size=filter_size); m = MaxPool2d(kernel_size=filter_size) </code>, where <code> i </code> and <code> o</code> are integers and <code>filter_size</code> can either be an integer (for square filters) or a tuple (for non-square filters i.e. (2, 3) for 2x3 filter).\n",
        "Train it on MNIST for one epoch, using <code>run_pytorch_cnn_mnist</code> (this may take a little while).  What is the accuracy on the validation set?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-sPW8xKF7EZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layers = None  # Your code\n",
        "\n",
        "run_pytorch_cnn_mnist(train, validation, layers, 1, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjmqgGvIDFiS",
        "colab_type": "text"
      },
      "source": [
        "<b> 3K)</b> Now, let's compare the performance of a fully connected model and a CNN on data where the characters have been shifted randomly so that they are no longer centered.  \n",
        "\n",
        "You can build such a data set by calling: <code>train_20, validation_20 = get_MNIST_data(shift=20)</code>. Remember to scale it appropriately.\n",
        "\n",
        "<b>Note that each image is now 48x48, so you will need to change your layer definitions (size after Flatten will be 6400)</b>.\n",
        "Run your two-hidden-layer FC architecture from above (problem 3I) on this data and then run the CNN architecture from above (problem 3J), both for one epoch. Report your results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvfiyrN9Gf7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_20, validation_20 = get_MNIST_data(shift=20)\n",
        "\n",
        "# Scale the images\n",
        "train_20 = (None, train_20[1])  # Your code\n",
        "validation_20 = (None, validation_20[1])  # Your code\n",
        "\n",
        "\n",
        "layers_fc = None  # Your code\n",
        "\n",
        "run_pytorch_fc_mnist(train_20, validation_20, layers_fc, 1, verbose=True)\n",
        "\n",
        "layers_cnn = None  # Your code\n",
        "\n",
        "run_pytorch_cnn_mnist(train_20, validation_20, layers_cnn, 1, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL4ZfEWIBB4B",
        "colab_type": "text"
      },
      "source": [
        "# 4. Raining Cats and Dogs\n",
        "\n",
        "In this problem, we are going to explore how a model trained on a particular dataset behaves in the general population. We will use the following functions (and [generator](https://wiki.python.org/moin/Generators))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7JiISTWFi0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_images(directory, imdir='./'):\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    for i in os.listdir(os.path.join(imdir, directory)):\n",
        "        img = resize(imread(os.path.join(imdir, directory, i)), (224, 224), anti_aliasing=True)\n",
        "        imgs.append(np.moveaxis(img, 2, 0))\n",
        "        if 'cat' in i:\n",
        "            labels.append(0)\n",
        "        else:\n",
        "            labels.append(1)\n",
        "    imgs = np.array(imgs)\n",
        "    return imgs, labels\n",
        "\n",
        "def data_gen(train_images, train_labels, batch_size, eval=True):\n",
        "    all_idxs = np.arange(len(train_labels))\n",
        "    idxs = np.random.shuffle(all_idxs)\n",
        "    i = 0\n",
        "    while i * batch_size + batch_size < train_images.shape[0]:\n",
        "        samples = train_images[i*batch_size: (i+1)*batch_size]\n",
        "        sample_labels = train_labels[i*batch_size: (i+1)*batch_size]\n",
        "        i += 1\n",
        "        \n",
        "        yield torch.tensor(samples, dtype=torch.float), torch.tensor(sample_labels, dtype=torch.long)\n",
        "    if eval:\n",
        "        samples = train_images[(i)*batch_size:]\n",
        "        sample_labels = train_labels[(i)*batch_size:]\n",
        "        yield torch.tensor(samples, dtype=torch.float), torch.tensor(sample_labels, dtype=torch.long)\n",
        "def postproc_output(out):\n",
        "  sm = torch.nn.Softmax(dim=1)\n",
        "  return sm(out).detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-07d6KNTFjB_",
        "colab_type": "text"
      },
      "source": [
        "**4A)** Write code to evaluate the model on each of the three test sets, following this pseudocode:\n",
        "<ol>\n",
        "    <li> load model \n",
        "    <ul>\n",
        "      <li> <tt> squeezenet_trained_cats_v_dogs.pt</tt> contains the <tt>state_dict</tt> of the model\n",
        "        <li> you will need to instantiate the model architecture first by running:\n",
        "          <tt>model = torchvision.models.squeezenet1_1(num_classes=2)</tt>\n",
        "        <li> then use <a href=\"https://pytorch.org/docs/stable/nn.html?highlight=load_state_dict#torch.nn.Module.load_state_dict\"><tt> model.load_state_dict</tt> </a>; make sure to use the parameter <tt>map_location=torch.device('cpu')</tt> when you use <tt>torch.load</tt>. It might also be helpful to read about the general workflow of <a href=\"https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference\"> saving and loading trained models in Pytorch </a>\n",
        "      <li> make sure that you are in evaluation model by using <tt> model.eval() </tt>\n",
        "    </ul>\n",
        "  <li> load data [use our function <tt>load_images</tt>]\n",
        "  <li> for each batch of data [use our generator <tt>data_gen</tt>; note that the batch size doesn't really matter here except to keep from having to multiply matrices that are too large]:\n",
        "    <ul>\n",
        "      <li> pass the batch of <tt>data</tt> through the model using <tt>model(data)</tt>\n",
        "        <li> convert the predictions of the model to guesses (after softmax) [use our function <tt>postproc_output</tt>]\n",
        "      <li> compare the guesses to the actual <tt>labels</tt>\n",
        "    </ul>\n",
        "    <li> total the accuracy\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIG9E0NgBjo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(model_path, data_path):\n",
        "    '''takes in a path to a model and a set of images\n",
        "        evaluates the model on that set of images'''\n",
        "    pass #Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "111yzeXLB2lH",
        "colab_type": "text"
      },
      "source": [
        "Calculate the performance on each of the test sets.\n"
      ]
    }
  ]
}